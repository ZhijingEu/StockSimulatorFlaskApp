{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_datareader import data\n",
    "\n",
    "import numpy as np, numpy.random\n",
    "from numpy import mean\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from scipy.stats import norm \n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "from scipy import stats\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "import pmdarima\n",
    "import arch\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCENARIO 1 : SINGLE STOCK\n",
    "\n",
    "start_date = '2016-12-30'\n",
    "end_date='2019-06-11'\n",
    "forecast_end_date='2019-07-05'\n",
    "backtest_duration=0 #day vs last day of available data\n",
    "T=np.busday_count(end_date,forecast_end_date)+backtest_duration\n",
    "N=T+1\n",
    "\n",
    "percentile_range='P10_P90'\n",
    "symbols=['1023.KL']\n",
    "portfolioWeights=[1]\n",
    "\n",
    "portfolioValue=5000\n",
    "\n",
    "NoOfIterationsWeights=5000\n",
    "NoOfIterationsMC=100\n",
    "percentile_ranges=\"P10_P90\"\n",
    "riskfreerateannual=0.0081 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCENARIO 2 : SINGLE STOCK BACKTEST ZERO\n",
    "\n",
    "start_date = '2017-07-11'\n",
    "end_date='2020-05-08'\n",
    "forecast_end_date='2020-05-21'\n",
    "backtest_duration=20 #day vs last day of available data\n",
    "T=np.busday_count(end_date,forecast_end_date)+backtest_duration\n",
    "N=T+1\n",
    "\n",
    "percentile_range='P10_P90'\n",
    "symbols=['C09.SI']\n",
    "portfolioWeights=[1]\n",
    "\n",
    "portfolioValue=5000\n",
    "\n",
    "NoOfIterationsWeights=5000\n",
    "NoOfIterationsMC=100\n",
    "percentile_ranges=\"P10_P90\"\n",
    "riskfreerateannual=0.0081 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCENARIO 3 : MORE THAN ONE STOCK\n",
    "\n",
    "start_date = '2017-01-02'\n",
    "end_date='2020-12-10'\n",
    "forecast_end_date='2020-12-31'\n",
    "backtest_duration=30 #day vs last day of available data\n",
    "T=np.busday_count(end_date,forecast_end_date)+backtest_duration\n",
    "N=T+1\n",
    "\n",
    "symbols=['MDLZ','NFLX','IBM','FB']\n",
    "portfolioWeights=[0.3,0.3,0.2,0.2]\n",
    "\n",
    "portfolioValue=10000\n",
    "\n",
    "NoOfIterationsWeights=5000\n",
    "NoOfIterationsMC=100\n",
    "percentile_range=\"P10_P90\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCENARIO 4 : MORE THAN ONE STOCK\n",
    "\n",
    "start_date = '2017-01-02'\n",
    "end_date='2020-12-10'\n",
    "forecast_end_date='2020-12-31'\n",
    "backtest_duration=0 #day vs last day of available data\n",
    "T=np.busday_count(end_date,forecast_end_date)+backtest_duration\n",
    "N=T+1\n",
    "\n",
    "symbols=['FB','NVDA','AMZN','NFLX','GOOG']\n",
    "portfolioWeights=[0.3,0.1,0.2,0.2,0.2]\n",
    "\n",
    "portfolioValue=5000\n",
    "\n",
    "NoOfIterationsWeights=5000\n",
    "NoOfIterationsMC=100\n",
    "percentile_range=\"P10_P90\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based loosely on https://towardsdatascience.com/how-to-simulate-financial-portfolios-with-python-d0dc4b52a278\n",
    "\n",
    "def extract_prices(start_date,end_date,symbols,portfolioWeights,portfolioValue,backtestduration=0):\n",
    "    dim=len(symbols)\n",
    "    for symbol in symbols:\n",
    "        dfprices = data.DataReader(symbols, start=start_date, end=end_date, data_source='yahoo')\n",
    "        dfprices = dfprices[['Adj Close']]\n",
    "    dfprices.columns=[' '.join(col).strip() for col in dfprices.columns.values]\n",
    "\n",
    "    priceAtEndDate=[]\n",
    "    for symbol in symbols:\n",
    "        priceAtEndDate.append(dfprices[[f'Adj Close {symbol}']][-(backtestduration+1):].values[0][0])\n",
    "        \n",
    "    noOfShares=[]\n",
    "    portfolioValPerSymbol=[x * portfolioValue for x in portfolioWeights]\n",
    "    for i in range(0,len(symbols)):\n",
    "        noOfShares.append(portfolioValPerSymbol[i]/priceAtEndDate[i])\n",
    "    noOfShares=[round(element, 5) for element in noOfShares]\n",
    "    listOfColumns=dfprices.columns.tolist()   \n",
    "    dfprices[\"Adj Close Portfolio\"]=dfprices[listOfColumns].mul(noOfShares).sum(1)\n",
    "    \n",
    "    share_split_table=dfprices.tail(1).T\n",
    "    share_split_table=share_split_table.iloc[:-1]\n",
    "    share_split_table[\"Share\"]=symbols\n",
    "    share_split_table[\"No Of Shares\"]=noOfShares\n",
    "    share_split_table.columns=[\"Price At \"+end_date,\"Share Name\",\"No Of Shares\"]\n",
    "    share_split_table[\"Value At \"+end_date]=share_split_table[\"No Of Shares\"]*share_split_table[\"Price At \"+end_date]\n",
    "    share_split_table.index=share_split_table[\"Share Name\"]\n",
    "    share_split_table=share_split_table[[\"Share Name\",\"Price At \"+end_date,\"No Of Shares\",\"Value At \"+end_date]]\n",
    "    share_split_table=share_split_table.round(3)\n",
    "    share_split_table=share_split_table.append(share_split_table.sum(numeric_only=True), ignore_index=True)\n",
    "    share_split_table.at[len(symbols),'No Of Shares']=np.nan\n",
    "    share_split_table.at[len(symbols),'Price At '+end_date]=np.nan\n",
    "    share_split_table.at[len(symbols),'Share Name']=\"Portfolio\"\n",
    "    share_split_table[\"Weights\"]=portfolioWeights+[\"1\"]\n",
    "    share_split_table = share_split_table[['Share Name', 'Weights', 'Price At '+end_date, 'No Of Shares', \"Value At \"+end_date]] \n",
    "    \n",
    "    print(f\"Extracted {len(dfprices)} days worth of data for {len(symbols)} counters with {dfprices.isnull().sum().sum()} missing data\")\n",
    "    \n",
    "    return dfprices, noOfShares, share_split_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfprices, noOfShares, share_split_table = extract_prices(start_date,end_date,symbols,portfolioWeights,portfolioValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotprices(dfprices,symbols,imagecounter,targetfolder):\n",
    "    dfprices.plot(subplots=True, figsize=(15,7.5*len(symbols)))\n",
    "    plt.savefig(f'static/{targetfolder}/{imagecounter}_02adjclosingprices.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotprices(dfprices,symbols,\"05678\",\"analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotpiechart(symbols,portfolioWeights,imagecounter,targetfolder):    \n",
    "    labels = symbols\n",
    "    sizes = portfolioWeights\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.pie(portfolioWeights, labels=symbols, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    plt.title(\"Portfolio Weights\")\n",
    "    plt.savefig(f'static/{targetfolder}/{imagecounter}_01portfolioweights.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modified from https://medium.com/swlh/generating-candlestick-charts-from-scratch-ef6e1d3cf0e9\n",
    "\n",
    "# Function to draw candlestick\n",
    "def draw_candlestick(axis, data, color_up, color_down):\n",
    "    \n",
    "    # Check if stock closed higher or not\n",
    "    if data['Close'] > data['Open']:\n",
    "        color = color_up\n",
    "    else:\n",
    "        color = color_down\n",
    "\n",
    "    # Plot the candle wick\n",
    "    axis.plot([data['day_num'], data['day_num']], [data['Low'], data['High']], linewidth=2, color='black', solid_capstyle='round', zorder=2)\n",
    "    \n",
    "    # Draw the candle body\n",
    "    rect = mpl.patches.Rectangle((data['day_num'] - 0.5, data['Open']), 1.0, (data['Close'] - data['Open']), facecolor=color, edgecolor='black', linewidth=1, zorder=3)\n",
    "\n",
    "    # Add candle body to the axis\n",
    "    axis.add_patch(rect)\n",
    "    \n",
    "    # Return modified axis\n",
    "    return axis\n",
    "\n",
    "# Function to draw all candlesticks\n",
    "def draw_all_candlesticks(axis, data, color_up='white', color_down='black'):\n",
    "    for day in range(data.shape[0]):\n",
    "        axis = draw_candlestick(axis, data.iloc[day], color_up, color_down)\n",
    "    return axis\n",
    "\n",
    "def plot_candlesticks(symbols,start_date,end_date,imagecounter,targetfolder):\n",
    "    for i in range(0,len(symbols)):\n",
    "        tkr_str = str(symbols[i])\n",
    "        tkr_history = data.DataReader(tkr_str, start=start_date, end=end_date, data_source='yahoo')\n",
    "        tkr_history['Date']=tkr_history.index\n",
    "        base_date = tkr_history['Date'][0]\n",
    "        tkr_history['day_num'] = tkr_history['Date'].map(lambda date:(date - base_date).days)\n",
    "\n",
    "        # Create figure and axes\n",
    "        fig = plt.figure(figsize=(20, 10), facecolor='white')\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "        # Colors for candlesticks\n",
    "        colors = ['#00FF00', '#FF0000']\n",
    "\n",
    "        # Grid lines\n",
    "        ax.grid(linestyle='-', linewidth=4, color='white', zorder=1)\n",
    "\n",
    "        # Draw candlesticks\n",
    "        ax = draw_all_candlesticks(ax, tkr_history, colors[0], colors[1])\n",
    "\n",
    "        # Set ticks to every 5th day\n",
    "        ax.set_xticks(list(tkr_history['day_num'])[::15])\n",
    "        ax.set_xticklabels(list(tkr_history['Date'].dt.strftime('%Y-%m-%d'))[::15])\n",
    "        ax.tick_params(labelsize=14)\n",
    "        plt.xticks(rotation=50)\n",
    "\n",
    "#         # Add dollar signs\n",
    "#         formatter = mpl.ticker.FormatStrFormatter('$%.2f')\n",
    "#         ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "        # Append ticker symbol\n",
    "        ax.text(0, 1.05, tkr_str, va='baseline', ha='left', size=20, transform=ax.transAxes)\n",
    "\n",
    "        # Set axis limits\n",
    "        ax.set_xlim(-1, tkr_history['day_num'].iloc[-1] + 1)\n",
    "\n",
    "        # Show plot\n",
    "        plt.savefig(f'static/{targetfolder}/{imagecounter}_candlestick{i}.png')\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_candlesticks(symbols,start_date,end_date,\"05678\",\"candlesticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modified from https://tcoil.info/compute-bollinger-bands-for-stocks-with-python-and-pandas/\n",
    "\n",
    "    # n = smoothing length eg 20\n",
    "    # m = number of standard deviations away from MA eg 2\n",
    "    \n",
    "def bollinger_bands(start_date,end_date,symbol, n, m,i):\n",
    "\n",
    "    df=data.DataReader(symbol, start=start_date, end=end_date, data_source='yahoo')\n",
    "    \n",
    "    #typical price\n",
    "    TP = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "    # but we will use Adj close instead for now, depends\n",
    "    \n",
    "    datax = TP\n",
    "    #data = df['Adj Close']\n",
    "    \n",
    "    # takes one column from dataframe\n",
    "    B_MA = pd.Series((datax.rolling(n, min_periods=n).mean()), name='B_MA')\n",
    "    sigma = datax.rolling(n, min_periods=n).std() \n",
    "    \n",
    "    BU = pd.Series((B_MA + m * sigma), name='BU')\n",
    "    BL = pd.Series((B_MA - m * sigma), name='BL')\n",
    "    \n",
    "    df = df.join(B_MA)\n",
    "    df = df.join(BU)\n",
    "    df = df.join(BL)  \n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_bollingerbands(symbols,start_date,end_date,n,m,imagecounter,targetfolder):\n",
    "    for i in range(0,len(symbols)):\n",
    "        df=bollinger_bands(start_date,end_date,str(symbols[i]), n, m,i)\n",
    "        # plot correspondingRSI values and significant levels\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.title(symbols[i]+': Bollinger Bands For Smoothing Length Of '+str(n)+' Days & '+str(m)+' Std Devs From MA')\n",
    "        plt.plot(df.index, df['Adj Close'])\n",
    "        plt.plot(df.index, df['BU'], alpha=0.3)\n",
    "        plt.plot(df.index, df['BL'], alpha=0.3)\n",
    "        plt.plot(df.index, df['B_MA'], alpha=0.3)\n",
    "        plt.fill_between(df.index, df['BU'], df['BL'], color='grey', alpha=0.1)\n",
    "        plt.savefig(f'static/{targetfolder}/{imagecounter}_bollingerband{i}.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bollingerbands(symbols,start_date,end_date,20,2,'12345','bollingerbands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modified from https://stackoverflow.com/questions/20526414/relative-strength-index-in-python-pandas\n",
    "\n",
    "def calcRSI(start_date,end_date,symbol,time_window,RSItype=\"EWMA\"):\n",
    "\n",
    "    df=data.DataReader(symbol, start=start_date, end=end_date, data_source='yahoo')\n",
    "    \n",
    "    diff = df[\"Adj Close\"].diff(1).dropna()        # diff in one field(one day)\n",
    "\n",
    "    #this preservers dimensions off diff values\n",
    "    up_chg = 0 * diff\n",
    "    down_chg = 0 * diff\n",
    "    \n",
    "    # up change is equal to the positive difference, otherwise equal to zero\n",
    "    up_chg[diff > 0] = diff[ diff>0 ]\n",
    "    \n",
    "    # down change is equal to negative deifference, otherwise equal to zero\n",
    "    down_chg[diff < 0] = diff[ diff < 0 ]\n",
    "    \n",
    "    # check pandas documentation for ewm\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html\n",
    "    # values are related to exponential decay\n",
    "    # we set com=time_window-1 so we get decay alpha=1/time_window\n",
    "    if RSItype==\"EWMA\":\n",
    "        up_chg_avg   = up_chg.ewm(com=time_window-1 , min_periods=time_window, adjust=False).mean()\n",
    "        down_chg_avg = down_chg.ewm(com=time_window-1 , min_periods=time_window, adjust=False).mean()\n",
    "    \n",
    "    elif RSItype==\"SMA\":\n",
    "    # Calculate the SMA\n",
    "        up_chg_avg = up_chg.rolling(time_window).mean()\n",
    "        down_chg_avg = down_chg.abs().rolling(time_window).mean()\n",
    "    \n",
    "    rs = abs(up_chg_avg/down_chg_avg)\n",
    "    rsi = 100 - 100/(1+rs)\n",
    "    df[\"RSI\"]=rsi\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_RSI(symbols,start_date,end_date,time_window,RSItype,imagecounter,targetfolder):\n",
    "    for i in range(0,len(symbols)):\n",
    "        dfrsi=calcRSI(start_date,end_date,str(symbols[i]),time_window,RSItype)\n",
    "        plt.figure(figsize=(15,5))\n",
    "        dfrsi['Adj Close'][time_window::].plot()\n",
    "        plt.title(str(symbols[i])+\": Adj Close Price\")\n",
    "        plt.savefig(f'static/{targetfolder}/{imagecounter}_{i}adjclosingprice.png')\n",
    "        plt.show\n",
    "        \n",
    "        plt.figure(figsize=(15,5))\n",
    "        dfrsi[\"RSI\"].plot()\n",
    "        plt.axhline(0, linestyle='--', alpha=0.1)\n",
    "        plt.axhline(20, linestyle='--', alpha=0.5)\n",
    "        plt.axhline(30, linestyle='--')\n",
    "        plt.axhline(70, linestyle='--')\n",
    "        plt.axhline(80, linestyle='--', alpha=0.5)\n",
    "        plt.axhline(100, linestyle='--', alpha=0.1)\n",
    "        plt.title(str(symbols[i])+\": RSI Plot: \"+RSItype+\" With Period Of \"+str(time_window)+\" Days\")\n",
    "        plt.savefig(f'static/{targetfolder}/{imagecounter}_{i}relativestrengthindex.png')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When RSI values are:\n",
    "\n",
    "# above 70, asset is considered overbought (overvalued)\n",
    "# below 30, asset is considered oversold (undervalued)\n",
    "# Even better is to look at divergence between price and RSI values:\n",
    "\n",
    "# bullish divergence: price is trending down, RSI values are increasing (possible Long entry)\n",
    "# bearish divergence: price is th=rending up, RSI values are decreasing (possible Short entry)\n",
    "\n",
    "\n",
    "plot_RSI(symbols,start_date,end_date,14,\"EWMA\",\"3456\",\"relativestrengthindex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMACD(start_date,end_date,symbol,timeperiod1,timeperiod2,timeperiod3,imagecounter,targetfolder):\n",
    "    df=data.DataReader(symbol, start=start_date, end=end_date, data_source='yahoo')\n",
    "    \n",
    "    df[\"EWMA \"+str(timeperiod1)+\" Days\"]=df[\"Adj Close\"].ewm(span=timeperiod1,min_periods=timeperiod1,adjust=False,ignore_na=False).mean()\n",
    "    df[\"EWMA \"+str(timeperiod2)+\" Days\"]=df[\"Adj Close\"].ewm(span=timeperiod2,min_periods=timeperiod2,adjust=False,ignore_na=False).mean()\n",
    "    df[\"MACD\"]=df[\"EWMA \"+str(timeperiod1)+\" Days\"]-df[\"EWMA \"+str(timeperiod2)+\" Days\"]\n",
    "    df[\"Signal Line\"]=df[\"MACD\"].ewm(span=timeperiod3,min_periods=timeperiod3,adjust=False,ignore_na=False).mean()\n",
    "    \n",
    "    \n",
    "    df[['Adj Close',\"EWMA \"+str(timeperiod1)+\" Days\",\"EWMA \"+str(timeperiod2)+\" Days\"]].plot(figsize=(15,5))\n",
    "    plt.title(symbol+\": MACD (\"+str(timeperiod1)+\",\"+str(timeperiod2)+\",\"+str(timeperiod3)+\") - EWMAs & Adj Close Price\")\n",
    "    plt.savefig(f'static/{targetfolder}/{imagecounter}_{symbol}_1_MACD.png')\n",
    "    \n",
    "    df[[\"MACD\",\"Signal Line\"]].plot(figsize=(15,5))\n",
    "    plt.title(symbol+\": MACD (\"+str(timeperiod1)+\",\"+str(timeperiod2)+\",\"+str(timeperiod3)+\") - MACD & Signal Line\")\n",
    "    plt.savefig(f'static/{targetfolder}/{imagecounter}_{symbol}_2_MACD.png')\n",
    "    \n",
    "    df[\"Histogram\"]=df[\"MACD\"]-df[\"Signal Line\"]\n",
    "    df.plot.bar(y='Histogram',figsize=(15,5))\n",
    "    tick_spacing = 28\n",
    "    plt.gca().xaxis.set_major_locator(plt.AutoLocator())\n",
    "    plt.title(symbol+\": MACD (\"+str(timeperiod1)+\",\"+str(timeperiod2)+\",\"+str(timeperiod3)+\") - MACD Histogram\")\n",
    "    \n",
    "    #plt.savefig(f'static/{targetfolder}/{imagecounter}_{symbol}_3_MACD.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_MACD(start_date,end_date,symbols,timeperiod1,timeperiod2,timeperiod3,imagecounter,targetfolder):\n",
    "    for symbol in symbols:\n",
    "        calcMACD(start_date,end_date,symbol,timeperiod1,timeperiod2,timeperiod3,imagecounter,targetfolder)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_returns(dfprices,symbols):\n",
    "    dfreturns=pd.DataFrame()\n",
    "    columns = list(dfprices) \n",
    "    mean=[]\n",
    "    stdev=[]\n",
    "    for column in columns:\n",
    "        dfreturns[f'Log Daily Returns {column}']=np.log(dfprices[column]).diff()\n",
    "        mean.append(dfreturns[f'Log Daily Returns {column}'][1:].mean())\n",
    "        stdev.append(dfreturns[f'Log Daily Returns {column}'][1:].std())\n",
    "    dfreturns=dfreturns.dropna()\n",
    "    \n",
    "    if len(dfreturns.columns)==1:\n",
    "        df_mean_stdev=pd.DataFrame(list(zip(symbols,mean,stdev)),columns =['Stock', 'Mean Log Daily Return','StdDev Log Daily Return']) \n",
    "    else:\n",
    "        df_mean_stdev=pd.DataFrame(list(zip(symbols+[\"Portfolio\"],mean,stdev)),columns =['Stock', 'Mean Log Daily Return','StdDev Log Daily Return'])\n",
    "    \n",
    "    return dfreturns ,df_mean_stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfreturns ,df_mean_stdev=calc_returns(dfprices,symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlations which are within the blue bands are not statistically significant\n",
    "def plotACFPACF(series,imagecounter,targetfolder):\n",
    "    \n",
    "    for i in range(0,len(series.columns)):\n",
    "        \n",
    "        plt.figure(figsize=(15,5))\n",
    "        Prices=series[series.columns[i]].plot()\n",
    "        plt.title(series.columns[i])\n",
    "        plt.savefig(f'static/{targetfolder}/{imagecounter}_{i}_Price.png')\n",
    "        \n",
    "        fig1, ax1 = plt.subplots(figsize=(15, 5))\n",
    "        ACFplot=plot_acf(series[series.columns[i]], lags=30, ax=ax1)\n",
    "        plt.title(series.columns[i]+' ACF')\n",
    "        plt.savefig(f'static/{targetfolder}/{imagecounter}_{i}_PriceACF.png')\n",
    "        \n",
    "        fig2, ax2 = plt.subplots(figsize=(15, 5))\n",
    "        PACFplot=plot_pacf(series[series.columns[i]], lags=30,ax=ax2)\n",
    "        plt.title(series.columns[i]+' PACF')\n",
    "        plt.savefig(f'static/{targetfolder}/{imagecounter}_{i}_PricePACF.png')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfreturns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-805cbebd608d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplotACFPACF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfreturns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"125435\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"ACFPACF\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dfreturns' is not defined"
     ]
    }
   ],
   "source": [
    "plotACFPACF(dfreturns,\"125435\",\"ACFPACF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertReturnsToPrices(dfreturns,startingrefprice):\n",
    "    stockprices=pd.DataFrame()\n",
    "    stockprices=np.exp(dfreturns)\n",
    "    stockprices=stockprices.cumprod()\n",
    "    stockprices=stockprices.mul(startingrefprice.values)\n",
    "    stockprices.columns=stockprices.columns.str.lstrip('Log Daily Returns ')\n",
    "    firstrow=pd.DataFrame(startingrefprice)\n",
    "    stockprices=pd.concat([firstrow,stockprices])\n",
    "    return stockprices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotreturns(dfreturns,imagecounter,targetfolder):\n",
    "    dfreturns.plot(subplots=True, figsize=(15,7.5*len(dfreturns.columns)))\n",
    "    plt.savefig(f'static/{targetfolder}/{imagecounter}_03Dailyreturns.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotreturns(dfreturns,\"05678\",\"analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfreturns.corr().round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareStartMidEnd(dfreturns,df_mean_stdev):\n",
    "\n",
    "    #print ('Size of dataFrame=', len(dfreturns.index))\n",
    "    desired_number_of_groups = 3\n",
    "    group_size = int(len(dfreturns.index) / (desired_number_of_groups))\n",
    "    #print(\"group_size=\", group_size)\n",
    "    remainder_size = len(dfreturns.index) % group_size\n",
    "    #print(\"remainder_size=\", remainder_size)\n",
    "    df_split_list = [dfreturns.iloc[i:i + group_size] for i in range(0, len(dfreturns) - group_size + 1, group_size)]\n",
    "    #print(\"Number of split_dataframes=\", len(df_split_list))\n",
    "    if remainder_size > 0:\n",
    "        df_remainder = dfreturns.iloc[-remainder_size:len(dfreturns.index)]\n",
    "        df_split_list.append(df_remainder)\n",
    "    #print(\"Revised Number of split_dataframes=\", len(df_split_list))\n",
    "    #print(\"Splitting complete, verifying counts\")\n",
    "\n",
    "    count_all_rows_after_split = 0\n",
    "    for index, split_df in enumerate(df_split_list):\n",
    "        #print(\"split_df:\", index, \" size=\", len(split_df.index))\n",
    "        count_all_rows_after_split += len(split_df.index)\n",
    "\n",
    "    if count_all_rows_after_split != len(dfreturns.index):\n",
    "        raise Exception('rows_after_split = ', count_all_rows_after_split,\" but original CSV DataFrame has count =\", len(dfreturns.index))\n",
    "\n",
    "    columns =['Stock','Start Mean', 'Start StdDev','Middle Mean','Middle StdDev','End Mean','End StdDev']\n",
    "    \n",
    "    anothertable=[]\n",
    "    for i in range(0,len(dfreturns.columns)):\n",
    "        boxplotsplit=pd.DataFrame()\n",
    "        boxplotsplit[\"Start\"]=df_split_list[0].iloc[:,i].values\n",
    "        boxplotsplit[\"Middle\"]=df_split_list[1].iloc[:,i].values\n",
    "        boxplotsplit[\"End\"]=df_split_list[2].iloc[:,i].values\n",
    "        #means = [boxplotsplit[\"Start\"].mean(),boxplotsplit[\"Middle\"].mean(),boxplotsplit[\"End\"].mean()]\n",
    "        #std =  [boxplotsplit[\"Start\"].std(),boxplotsplit[\"Middle\"].std(),boxplotsplit[\"End\"].std()]\n",
    "        meanstdev=[dfreturns.columns[i],boxplotsplit[\"Start\"].mean(),boxplotsplit[\"Start\"].std(),\\\n",
    "                   boxplotsplit[\"Middle\"].mean(),boxplotsplit[\"Middle\"].std(),\\\n",
    "                   boxplotsplit[\"End\"].mean(),boxplotsplit[\"End\"].std()]\n",
    "        \n",
    "        anothertable.append(meanstdev) \n",
    "        \n",
    "    yetanothertable=pd.DataFrame(anothertable,columns=columns)\n",
    "    \n",
    "    yetanothertable[\"Overall Mean\"]=df_mean_stdev[\"Mean Log Daily Return\"]\n",
    "    yetanothertable[\"Overall StdDev\"]=df_mean_stdev[\"StdDev Log Daily Return\"]\n",
    "    yetanothertable=yetanothertable[['Stock','Overall Mean', 'Start Mean','Middle Mean','End Mean',\\\n",
    "                                     'Overall StdDev','Start StdDev','Middle StdDev','End StdDev']]\n",
    "    \n",
    "    return yetanothertable\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StartMidEndMeanStdev=compareStartMidEnd(dfreturns,df_mean_stdev)\n",
    "StartMidEndMeanStdev.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_test_normal(dfreturns,symbols,imagecounter,targetfolder):\n",
    "    \n",
    "    columnlist=dfreturns.columns\n",
    "    KSTestResults=[]\n",
    "    KSPValResults=[]\n",
    "    for column in columnlist:\n",
    "        data=(dfreturns[column])\n",
    "        normed_data=(data-dfreturns[column].mean())/dfreturns[column].std()\n",
    "        KSTestResults.append(kstest(normed_data, 'norm')[0])\n",
    "        KSPValResults.append(kstest(normed_data, 'norm')[1])\n",
    "    if len(dfreturns.columns)==1:\n",
    "        KSTestResultsDF=pd.DataFrame([dfreturns.columns,KSTestResults,KSPValResults]).T\n",
    "    else:\n",
    "        KSTestResultsDF=pd.DataFrame([dfreturns.columns,KSTestResults,KSPValResults]).T\n",
    "    KSTestResultsDF.columns=[\"Share Name\",\"KS Test Statistic\",\"KS Test P-Value\"]\n",
    "    KSTestResultsDF[\"Accept/Reject At 5% Signif Lvl\"]=np.where(KSTestResultsDF['KS Test P-Value']> 0.05, \"Data looks normal (Fail to reject H0)\", \"Data does NOT look normal (Reject H0)\")\n",
    "    \n",
    "    ShapiroWilkTestStats=[]\n",
    "    ShapiroWilkTestPValue=[]\n",
    "    for column in columnlist:\n",
    "        data=(dfreturns[column])\n",
    "        stat, pvalue = shapiro(data)\n",
    "        ShapiroWilkTestStats.append(stat)\n",
    "        ShapiroWilkTestPValue.append(round(pvalue,6))\n",
    "\n",
    "    ShapiroWilkTestResultsDF=pd.DataFrame([ShapiroWilkTestStats,ShapiroWilkTestPValue]).T\n",
    "    if len(dfreturns.columns)==1:\n",
    "        ShapiroWilkTestResultsDF[\"Share Name\"]=dfreturns.columns\n",
    "    else:\n",
    "        ShapiroWilkTestResultsDF[\"Share Name\"]=dfreturns.columns\n",
    "    ShapiroWilkTestResultsDF.columns=[\"SW Test Statistic\",\"SW Test P-Value\",\"Share Name\"]\n",
    "    ShapiroWilkTestResultsDF=ShapiroWilkTestResultsDF[[\"Share Name\",\"SW Test Statistic\",\"SW Test P-Value\"]]\n",
    "    ShapiroWilkTestResultsDF[\"Accept/Reject At 5% Signif Lvl\"]=np.where(ShapiroWilkTestResultsDF['SW Test P-Value']> 0.05, \"Data looks normal (Fail to reject H0)\", \"Data does NOT look normal (Reject H0)\")\n",
    "\n",
    "    \n",
    "    ADFTestStats=[]\n",
    "    ADFTestCritValue=[]\n",
    "    for column in columnlist:\n",
    "        data=(dfreturns[column])        \n",
    "        ADFTest_result = adfuller(data)\n",
    "        ADFTestStats.append(ADFTest_result[0])\n",
    "        ADFTestCritValue.append(ADFTest_result[4]['5%'])\n",
    "\n",
    "    ADFTestResultsDF=pd.DataFrame([ADFTestStats,ADFTestCritValue]).T\n",
    "    ADFTestResultsDF[\"Share Name\"]=dfreturns.columns\n",
    "    ADFTestResultsDF.columns=[\"ADF Test Statistic\",\"ADF Test Stat Crit Value At 5% Signif Lvl\",\"Share Name\"]\n",
    "    ADFTestResultsDF=ADFTestResultsDF[[\"Share Name\",\"ADF Test Statistic\",\"ADF Test Stat Crit Value At 5% Signif Lvl\"]]\n",
    "    ADFTestResultsDF[\"Accept/Reject At 5% Signif Lvl\"]=np.where(ADFTestResultsDF['ADF Test Statistic']> ADFTestResultsDF['ADF Test Stat Crit Value At 5% Signif Lvl'] , \"Time series is NON-stationary (Reject H0)\", \"Time series is stationary (Fail To Reject H0)\")  \n",
    "        \n",
    "    skewArray=[]\n",
    "    kurtosisArray=[]\n",
    "    for column in columnlist:\n",
    "        data=(dfreturns[column])\n",
    "        skewArray.append(skew(data))\n",
    "        kurtosisArray.append(kurtosis(data))\n",
    "        mu = np.mean(data)\n",
    "        sigma = np.std(data)\n",
    "        plt.figure(figsize = (15, 5))\n",
    "        plt.hist(data, bins=50, density=True, alpha=0.6, color='g')\n",
    "        # Plot the PDF.\n",
    "        xmin, xmax = plt.xlim()\n",
    "        x = np.linspace(xmin, xmax, 100)\n",
    "        p = norm.pdf(x, mu, sigma)\n",
    "        plt.plot(x, p, 'k', linewidth=2)\n",
    "        title = column+\" Histogram vs Best Fit Normal Distribution Mu = %.3f,  Sigma = %.3f\" % (mu, sigma)\n",
    "        plt.title(title)\n",
    "        plt.savefig(f'static/{targetfolder}/{imagecounter}_04histogram{column}.png')\n",
    "    \n",
    "    Kurtosis_Skew=pd.DataFrame(list(zip(skewArray, kurtosisArray)), \n",
    "               index=columnlist,columns =['Skew','Kurtosis']) \n",
    "    \n",
    "#     If the skewness is between -0.5 and 0.5, the data are fairly symmetrical.\n",
    "#     If the skewness is between -1 and – 0.5 or between 0.5 and 1, the data are moderately skewed.\n",
    "#     If the skewness is less than -1 or greater than 1, the data are highly skewed.\n",
    "    def skewconditions(s):\n",
    "        if (s['Skew'] > 1) :\n",
    "            return \"Highly Positively Skewed\"\n",
    "        elif (s['Skew'] <= 1) and (s['Skew'] > 0.5) :\n",
    "            return \"Moderately Positively Skewed\"\n",
    "        elif (s['Skew'] <= 0.5) and (s['Skew'] > -0.5) :\n",
    "            return \"Fairly Symmetrical\"\n",
    "        elif (s['Skew'] <= -0.5) and (s['Skew'] > -1.0) :\n",
    "            return \"Moderately Negatively Skewed\"\n",
    "        elif (s['Skew'] <= -1.0) :\n",
    "            return \"Highly Negatively Skewed\"\n",
    "    \n",
    "#     For kurtosis, the general guideline is that if the number is greater than +1, the distribution is too peaked. \n",
    "#     Likewise, a kurtosis of less than –1 indicates a distribution that is too flat\n",
    "    def kurtosisconditions(s):\n",
    "        if (s['Kurtosis'] > 3) :\n",
    "            return \"Leptokurtic:Peaked & Fat Tailed\"\n",
    "        elif (s['Kurtosis'] <-3):\n",
    "            return \"Platykurtic:Flat & Thin Tailed\"\n",
    "        else :\n",
    "            return \"Mesokurtic\"\n",
    "    \n",
    "    Kurtosis_Skew['Skew Description'] = Kurtosis_Skew.apply(skewconditions, axis=1)\n",
    "    Kurtosis_Skew['Kurtosis Description'] = Kurtosis_Skew.apply(kurtosisconditions, axis=1)\n",
    "    \n",
    "    for i in range(0,len(columnlist)):\n",
    "        data=(dfreturns.iloc[:, i])\n",
    "        res = stats.probplot(data, dist=\"norm\")\n",
    "        xxx=pd.DataFrame(list(zip(res[0][0],res[0][1])),columns =[\"Theoretical Quantiles\",\"Ordered Values\"]) \n",
    "        xxx.plot.scatter(x=\"Theoretical Quantiles\",y=\"Ordered Values\",title=columnlist[i],figsize = (7.5, 5))\n",
    "        plt.plot(xxx[\"Theoretical Quantiles\"], res[1][0]*xxx[\"Theoretical Quantiles\"]+res[1][1])\n",
    "        plt.savefig(f'static/{targetfolder}/{imagecounter}_05qqplot{i}.png')\n",
    "    \n",
    "    return KSTestResultsDF, ShapiroWilkTestResultsDF, Kurtosis_Skew, ADFTestResultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KSTestResultsDF, ShapiroWilkTestResultsDF , Kurtosis_Skew, ADFTestResultsDF = fit_test_normal(dfreturns,symbols,\"05678\",\"analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(KSTestResultsDF)\n",
    "display(ShapiroWilkTestResultsDF)\n",
    "display(Kurtosis_Skew)\n",
    "display(ADFTestResultsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_w_replc_singleval(dfreturns):\n",
    "    columns=dfreturns.columns\n",
    "    singlesample=pd.DataFrame(dfreturns.values[np.random.randint(len(dfreturns), size=1)], columns=columns)\n",
    "    return singlesample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapforecast(dfreturns,T):\n",
    "    columnlist=dfreturns.columns\n",
    "    X=[]\n",
    "    for i in range(0,T):\n",
    "        X.append(bootstrap_w_replc_singleval(dfreturns).values.tolist()[0])\n",
    "    Y=pd.DataFrame(X)\n",
    "    Y.columns=columnlist\n",
    "    Y.loc[-1] = [0]*len(columnlist)  # adding a row\n",
    "    Y.index = Y.index + 1  # shifting index\n",
    "    Y = Y.sort_index()  # sorting by index\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from https://stackoverflow.com/questions/10939213/how-can-i-calculate-the-nearest-positive-semi-definite-matrix\n",
    "\n",
    "#import numpy as np,numpy.linalg\n",
    "\n",
    "def _getAplus(A):\n",
    "    eigval, eigvec = np.linalg.eig(A)\n",
    "    Q = np.matrix(eigvec)\n",
    "    xdiag = np.matrix(np.diag(np.maximum(eigval, 0)))\n",
    "    return Q*xdiag*Q.T\n",
    "\n",
    "def _getPs(A, W=None):\n",
    "    W05 = np.matrix(W**.5)\n",
    "    return  W05.I * _getAplus(W05 * A * W05) * W05.I\n",
    "\n",
    "def _getPu(A, W=None):\n",
    "    Aret = np.array(A.copy())\n",
    "    Aret[W > 0] = np.array(W)[W > 0]\n",
    "    return np.matrix(Aret)\n",
    "\n",
    "def nearPD(A, nit=10):\n",
    "    n = A.shape[0]\n",
    "    W = np.identity(n) \n",
    "# W is the matrix used for the norm (assumed to be Identity matrix here)\n",
    "# the algorithm should work for any diagonal W\n",
    "    deltaS = 0\n",
    "    Yk = A.copy()\n",
    "    for k in range(nit):\n",
    "        Rk = Yk - deltaS\n",
    "        Xk = _getPs(Rk, W=W)\n",
    "        deltaS = Xk - Rk\n",
    "        Yk = _getPu(Xk, W=W)\n",
    "    return Yk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_covar(dfreturns):  \n",
    "    try:\n",
    "        returns=[]\n",
    "        arrOfReturns=[]\n",
    "        columns = list(dfreturns)\n",
    "        for column in columns:\n",
    "            returns=dfreturns[column].values.tolist()\n",
    "            arrOfReturns.append(returns)\n",
    "        Cov = np.cov(np.array(arrOfReturns))    \n",
    "        return Cov\n",
    "    except LinAlgError :\n",
    "        Cov = nearPD(np.array(arrOfReturns), nit=10)\n",
    "        print(\"WARNING -Original Covariance Matrix is NOT Positive Semi Definite And Has Been Adjusted To Allow For Cholesky Decomposition \")\n",
    "        return Cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVARIANCE_MATRIX=pd.DataFrame(create_covar(dfreturns),index=dfreturns.columns,columns=dfreturns.columns)\n",
    "COVARIANCE_MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.linalg.cholesky(COVARIANCE_MATRIX)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRELATION_MATRIX=dfreturns.corr()\n",
    "CORRELATION_MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBMsimulatorMultiVar(So, mu, sigma, Cov, T, N):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "\n",
    "    seed:   seed of simulation\n",
    "    So:     initial stocks' price\n",
    "    mu:     expected return\n",
    "    sigma:  volatility\n",
    "    Cov:    covariance matrix\n",
    "    T:      time period\n",
    "    N:      number of increments\n",
    "    \"\"\"\n",
    "\n",
    "    #np.random.seed(seed) turned off so Monte Carlo can be \"randomised\"\n",
    "    dim = np.size(So)\n",
    "    t = np.linspace(0., T, int(N))\n",
    "    A = np.linalg.cholesky(Cov)\n",
    "    S = np.zeros([dim, int(N)])\n",
    "    S[:, 0] = So\n",
    "    for i in range(1, int(N)):    \n",
    "        drift = (mu - 0.5 * sigma**2) * (t[i] - t[i-1])\n",
    "        Z = np.random.normal(0., 1., dim)\n",
    "        diffusion = np.matmul(A, Z) * (np.sqrt(t[i] - t[i-1]))\n",
    "        S[:, i] = S[:, i-1]*np.exp(drift + diffusion)\n",
    "    return S, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBMsimulatorUniVar(So, mu, sigma, T, N):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "\n",
    "    seed:   seed of simulation\n",
    "    So:     initial stocks' price\n",
    "    mu:     expected return\n",
    "    sigma:  volatility\n",
    "    Cov:    covariance matrix\n",
    "    T:      time period\n",
    "    N:      number of increments\n",
    "    \"\"\"\n",
    "\n",
    "    #np.random.seed(seed) turned off so Monte Carlo can be \"randomised\"\n",
    "    dim = np.size(So)\n",
    "    t = np.linspace(0., T, int(N))\n",
    "    S = np.zeros([dim, int(N)])\n",
    "    S[:, 0] = So\n",
    "    for i in range(1, int(N)):    \n",
    "        drift = (mu - 0.5 * sigma**2) * (t[i] - t[i-1])\n",
    "        Z = np.random.normal(0., 1., dim)\n",
    "        diffusion = sigma* Z * (np.sqrt(t[i] - t[i-1]))\n",
    "        S[:, i] = S[:, i-1]*np.exp(drift + diffusion)\n",
    "    return S, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRMSE(final,T,backtest_duration,dfprices):\n",
    "    xyz=final.tail(T)\n",
    "    xyz=xyz.head(backtest_duration)\n",
    "    xyz\n",
    "    qrs=pd.DataFrame(index=xyz.index)\n",
    "    for i in range(0,len(dfprices.columns)):\n",
    "        x=1+(len(dfprices.columns)-i)*-3\n",
    "        qrs[\"Actual \"+dfprices.columns[i]]=xyz.iloc[:,i]\n",
    "        qrs[\"P50 \"+dfprices.columns[i]]=xyz.iloc[:,x]\n",
    "    for i in range(0,len(dfprices.columns)):\n",
    "        x=i*2\n",
    "        qrs[\"RMSE \"+dfprices.columns[i]]=(qrs.iloc[:,x]-qrs.iloc[:,x+1])**2\n",
    "    qrs\n",
    "    RMSE=[]\n",
    "\n",
    "    for i in range(0,len(dfprices.columns)):\n",
    "        z=-(len(dfprices.columns)-i)\n",
    "        RMSE.append((qrs.iloc[:,z].mean())**0.5)\n",
    "    RMSE_DF=pd.DataFrame(RMSE,index=[dfprices.columns],columns=[\"RMSE For Backtest From \"+qrs.index[0].strftime(\"%Y-%m-%d\")+\" To \"+qrs.index[-1].strftime(\"%Y-%m-%d\")\\\n",
    "                                                               +\" (\"+str(len(qrs))+\" Days)\"])\n",
    "    return RMSE_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarlo_GBM(start_date,end_date,backtest_duration,percentile_range,symbols,\\\n",
    "                       portfolioWeights,portfolioValue,T,N,NoOfIterationsMC,imagecounter,targetfolder):\n",
    "    \n",
    "    forecastresults=pd.DataFrame()\n",
    "    percentiles=pd.DataFrame()\n",
    "    \n",
    "    extended_dates_future=[]\n",
    "    lowerpercentile=int(percentile_range[1:3])\n",
    "    upperpercentile=int(percentile_range[5:7])\n",
    " \n",
    "    plotpiechart(symbols,portfolioWeights,imagecounter,targetfolder)\n",
    "\n",
    "    if len(symbols)==1:\n",
    "        dfpricesFULL, noOfSharesFULL, share_split_tableFULL = extract_prices(start_date,end_date,symbols,portfolioWeights,portfolioValue)\n",
    "        backtest_end_date=dfpricesFULL.index[-(backtest_duration+1)].strftime(\"%Y-%m-%d\")\n",
    "        dfprices, noOfShares, share_split_table = extract_prices(start_date,backtest_end_date,symbols,portfolioWeights,portfolioValue)\n",
    "        dfprices[\"Adj Close Portfolio\"]=dfprices[list(dfprices.iloc[:,:-1].columns)].mul(noOfSharesFULL).sum(1)\n",
    "        \n",
    "    else:\n",
    "        dfpricesFULL, noOfSharesFULL, share_split_tableFULL = extract_prices(start_date,end_date,symbols,portfolioWeights,portfolioValue)\n",
    "        backtest_end_date=dfpricesFULL.index[-(backtest_duration+1)].strftime(\"%Y-%m-%d\")\n",
    "        dfprices, noOfShares, share_split_table = extract_prices(start_date,backtest_end_date,symbols,portfolioWeights,portfolioValue)\n",
    "        dfprices[\"Adj Close Portfolio\"]=dfprices[list(dfprices.iloc[:,:-1].columns)].mul(noOfSharesFULL).sum(1)\n",
    "                  \n",
    "    symbolsWPortfolio=symbols+[\"Portfolio\"]\n",
    "\n",
    "    dfreturns ,df_mean_stdev = calc_returns(dfprices,symbolsWPortfolio)\n",
    "    \n",
    "    S0=np.array(dfprices.tail(1).values.tolist()[0])\n",
    "    mu=np.array(df_mean_stdev[\"Mean Log Daily Return\"].values.tolist())\n",
    "    sigma=np.array(df_mean_stdev[\"StdDev Log Daily Return\"].values.tolist())   \n",
    "\n",
    "    backtestdateslist=(list((dfpricesFULL.tail(backtest_duration+1).index)))\n",
    "    backtestdates=[]\n",
    "    for i in backtestdateslist:\n",
    "        backtestdates.append(np.datetime64(datetime.strptime(str(i), '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")))\n",
    "    \n",
    "    for i in range(0,N-backtest_duration):\n",
    "        extended_dates_future.append(np.busday_offset(end_date, i, roll='forward'))\n",
    "        \n",
    "    extended_dates=backtestdates[0:len(backtestdates)-1]+extended_dates_future\n",
    "     \n",
    "    if len(symbols)==1:\n",
    "        for x in range(1,NoOfIterationsMC+1):\n",
    "            stocks, time = GBMsimulatorUniVar(S0, mu, sigma, T, N)\n",
    "            prediction=pd.DataFrame(stocks)\n",
    "            prediction=prediction.T\n",
    "            prediction.index=extended_dates\n",
    "            prediction.columns=dfprices.columns\n",
    "            prediction=prediction.add_prefix('Iter_'+str(x)+'_')\n",
    "            forecastresults=pd.concat([forecastresults, prediction], axis=1)\n",
    "        \n",
    "        for x in range(1,NoOfIterationsMC+1):\n",
    "            forecastresults[\"Iter_\"+str(x)+\"_Adj Close Portfolio\"]=forecastresults[\"Iter_\"+str(x)+\"_Adj Close \"+symbols[0]]*noOfSharesFULL\n",
    "\n",
    "    else:\n",
    "        Cov=create_covar(dfreturns)\n",
    "        for x in range(1,NoOfIterationsMC+1):\n",
    "            stocks, time = GBMsimulatorMultiVar(S0, mu, sigma, Cov, T, N)\n",
    "            prediction=pd.DataFrame(stocks)\n",
    "            prediction=prediction.T\n",
    "            prediction.index=extended_dates\n",
    "            prediction.columns=dfprices.columns\n",
    "            prediction=prediction.add_prefix('Iter_'+str(x)+'_')\n",
    "            forecastresults=pd.concat([forecastresults, prediction], axis=1)\n",
    "\n",
    "    for y in range(0,len(symbolsWPortfolio)):\n",
    "        percentiles[\"P\"+str(lowerpercentile)+\"_\"+symbolsWPortfolio[y]]=forecastresults.filter(regex=symbolsWPortfolio[y]).quantile(float(lowerpercentile)/100,1)\n",
    "        percentiles[\"P50_\"+symbolsWPortfolio[y]]=forecastresults.filter(regex=symbolsWPortfolio[y]).quantile(0.5,1)\n",
    "        percentiles[\"P\"+str(upperpercentile)+\"_\"+symbolsWPortfolio[y]]=forecastresults.filter(regex=symbolsWPortfolio[y]).quantile(float(upperpercentile)/100,1)\n",
    "\n",
    "        forecastresults=pd.concat([forecastresults,percentiles[[\"P\"+str(lowerpercentile)+\"_\"+symbolsWPortfolio[y],\"P50_\"+symbolsWPortfolio[y],\"P\"+str(upperpercentile)+\"_\"+symbolsWPortfolio[y]]]], axis=1, sort=False)\n",
    "    \n",
    "    final=pd.concat([dfpricesFULL,forecastresults], axis=1, sort=False)\n",
    "              \n",
    "    for z in range(0,len(symbolsWPortfolio)):\n",
    "        final.filter(regex=\"Adj Close \"+symbolsWPortfolio[z]).tail(60).plot(legend=False,figsize = (20, 5),title=symbolsWPortfolio[z]+\": Monte Carlo Simulations For \"+str(NoOfIterationsMC)+\" Iter-s\")\n",
    "        plt.axvline(x=end_date,linestyle='dashed')\n",
    "        plt.savefig(f'static/{targetfolder}/{imagecounter}_totaliterations{z}.png')\n",
    "        \n",
    "        percentileplot=pd.DataFrame()\n",
    "        percentileplot=pd.concat([final[\"Adj Close \"+symbolsWPortfolio[z]],final.filter(regex=\"P??_\"+symbolsWPortfolio[z])], axis=1, sort=False)\n",
    "        percentileplot.tail(60).plot(legend=True,figsize = (20, 5),title=symbolsWPortfolio[z]+\": Monte Carlo Simulations For \"+percentile_range+\" Range\")\n",
    "        plt.axvline(x=end_date,linestyle='dashed')\n",
    "        if NoOfIterationsMC>0:\n",
    "            plt.savefig(f'static/{targetfolder}/{imagecounter}_percentile{z}.png')\n",
    "  \n",
    "    ReturnsAtForecastEndDate=final.tail(1).iloc[:,-(len(symbolsWPortfolio))*3:].T\n",
    "    HelperTable=pd.concat([dfpricesFULL.tail(1).round(3).T]*(3)) \n",
    "    HelperTable[\"Sym\"]=HelperTable.index\n",
    "    HelperTable['Sym'] =pd.Categorical(HelperTable[\"Sym\"], list(dfprices.columns))\n",
    "    HelperTable=HelperTable.sort_values(['Sym'])\n",
    "    ReturnsAtForecastEndDate.insert(0, end_date, HelperTable.iloc[:,:-1].values)    \n",
    "           \n",
    "    ReturnsAtForecastEndDate[\"Returns Based On GBM\"]=round((ReturnsAtForecastEndDate.iloc[:, 1]/ReturnsAtForecastEndDate.iloc[:, 0]-1)*100,2)\n",
    "    \n",
    "    return final, share_split_tableFULL , dfreturns , df_mean_stdev , ReturnsAtForecastEndDate, dfprices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalGBM,b,c,d,ReturnsAtForecastEndDateGBM, dfprices=MonteCarlo_GBM(start_date,end_date,backtest_duration,percentile_range,symbols,\\\n",
    "                       portfolioWeights,portfolioValue,T,N,1,\"12354\",\"gbm_bootstrap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReturnsAtForecastEndDateGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarlo_Bootstrap(start_date,end_date,backtest_duration,percentile_range,symbols,\\\n",
    "                       portfolioWeights,portfolioValue,T,N,NoOfIterationsMC,imagecounter,targetfolder):\n",
    "    \n",
    "    forecastresults=pd.DataFrame()\n",
    "    percentiles=pd.DataFrame()\n",
    "    \n",
    "    extended_dates_future=[]\n",
    "    lowerpercentile=int(percentile_range[1:3])\n",
    "    upperpercentile=int(percentile_range[5:7])\n",
    "    \n",
    "    plotpiechart(symbols,portfolioWeights,imagecounter,targetfolder)\n",
    "\n",
    "    if len(symbols)==1:\n",
    "        dfpricesFULL, noOfSharesFULL, share_split_tableFULL = extract_prices(start_date,end_date,symbols,portfolioWeights,portfolioValue)\n",
    "        backtest_end_date=dfpricesFULL.index[-(backtest_duration+1)].strftime(\"%Y-%m-%d\")\n",
    "        dfprices, noOfShares, share_split_table = extract_prices(start_date,backtest_end_date,symbols,portfolioWeights,portfolioValue)\n",
    "        dfprices[\"Adj Close Portfolio\"]=dfprices[list(dfprices.iloc[:,:-1].columns)].mul(noOfSharesFULL).sum(1)\n",
    "        \n",
    "    else:\n",
    "        dfpricesFULL, noOfSharesFULL, share_split_tableFULL = extract_prices(start_date,end_date,symbols,portfolioWeights,portfolioValue)\n",
    "        backtest_end_date=dfpricesFULL.index[-(backtest_duration+1)].strftime(\"%Y-%m-%d\")\n",
    "        dfprices, noOfShares, share_split_table = extract_prices(start_date,backtest_end_date,symbols,portfolioWeights,portfolioValue)\n",
    "        dfprices[\"Adj Close Portfolio\"]=dfprices[list(dfprices.iloc[:,:-1].columns)].mul(noOfSharesFULL).sum(1)\n",
    "               \n",
    "    symbolsWPortfolio=symbols+[\"Portfolio\"]\n",
    "\n",
    "    dfreturns ,df_mean_stdev = calc_returns(dfprices,symbolsWPortfolio)\n",
    "\n",
    "    backtestdateslist=(list((dfpricesFULL.tail(backtest_duration+1).index)))\n",
    "    backtestdates=[]\n",
    "    for i in backtestdateslist:\n",
    "        backtestdates.append(np.datetime64(datetime.strptime(str(i), '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")))\n",
    "        \n",
    "    for i in range(0,N-backtest_duration):\n",
    "        extended_dates_future.append(np.busday_offset(end_date, i, roll='forward'))\n",
    "        \n",
    "    extended_dates=backtestdates[0:len(backtestdates)-1]+extended_dates_future\n",
    "\n",
    "    for x in range(1,NoOfIterationsMC+1):      \n",
    "        \n",
    "        futurereturns=bootstrapforecast(dfreturns,T)\n",
    "        futurereturns=np.exp(futurereturns)\n",
    "        futurereturns=futurereturns.cumprod()\n",
    "        stocks=pd.DataFrame()\n",
    "        for i in range(0,len(symbolsWPortfolio)):\n",
    "            futurereturns[str(i)+\"Price\"]=(futurereturns.iloc[:, i])*dfprices.tail(1).iloc[:, i][0]\n",
    "        stocks=futurereturns[futurereturns.columns[-len(symbolsWPortfolio):]] \n",
    "        stocks.columns=list(dfreturns.columns)\n",
    "\n",
    "        prediction=stocks\n",
    "        prediction.index=extended_dates\n",
    "        prediction.columns=dfprices.columns\n",
    "        prediction=prediction.add_prefix('Iter_'+str(x)+'_')\n",
    "        forecastresults=pd.concat([forecastresults,prediction], axis=1, sort=False)\n",
    "    \n",
    "    for y in range(0,len(symbolsWPortfolio)):\n",
    "        percentiles[\"P\"+str(lowerpercentile)+\"_\"+symbolsWPortfolio[y]]=forecastresults.filter(regex=symbolsWPortfolio[y]).quantile(float(lowerpercentile)/100,1)\n",
    "        percentiles[\"P50_\"+symbolsWPortfolio[y]]=forecastresults.filter(regex=symbolsWPortfolio[y]).quantile(0.5,1)\n",
    "        percentiles[\"P\"+str(upperpercentile)+\"_\"+symbolsWPortfolio[y]]=forecastresults.filter(regex=symbolsWPortfolio[y]).quantile(float(upperpercentile)/100,1)\n",
    "\n",
    "        forecastresults=pd.concat([forecastresults,percentiles[[\"P\"+str(lowerpercentile)+\"_\"+symbolsWPortfolio[y],\"P50_\"+symbolsWPortfolio[y],\"P\"+str(upperpercentile)+\"_\"+symbolsWPortfolio[y]]]], axis=1, sort=False)\n",
    "    \n",
    "    final=pd.concat([dfpricesFULL,forecastresults], axis=1, sort=False)\n",
    "    \n",
    "    for z in range(0,len(symbolsWPortfolio)):\n",
    "        final.filter(regex=\"Adj Close \"+symbolsWPortfolio[z]).tail(60).plot(legend=False,figsize = (20, 5),title=symbolsWPortfolio[z]+\": Monte Carlo Simulations For \"+str(NoOfIterationsMC)+\" Iter-s\")\n",
    "        plt.axvline(x=end_date,linestyle='dashed')\n",
    "        plt.savefig(f'static/{targetfolder}/{imagecounter}_totaliterations{z}.png')\n",
    "        \n",
    "        percentileplot=pd.DataFrame()\n",
    "        percentileplot=pd.concat([final[\"Adj Close \"+symbolsWPortfolio[z]],final.filter(regex=\"P??_\"+symbolsWPortfolio[z])], axis=1, sort=False)\n",
    "        percentileplot.tail(60).plot(legend=True,figsize = (20, 5),title=symbolsWPortfolio[z]+\": Monte Carlo Simulations For \"+percentile_range+\" Range\")\n",
    "        plt.axvline(x=end_date,linestyle='dashed')\n",
    "        if NoOfIterationsMC>1:\n",
    "            plt.savefig(f'static/{targetfolder}/{imagecounter}_percentile{z}.png')\n",
    "        \n",
    "    if len(symbols)==1:\n",
    "        ReturnsAtForecastEndDate=final.tail(1).iloc[:,-(len(symbolsWPortfolio))*3:].T\n",
    "        HelperTable=pd.concat([dfpricesFULL.tail(1).round(3).T]*(3))\n",
    "        HelperTable[\"Sym\"]=HelperTable.index\n",
    "        HelperTable['Sym'] =pd.Categorical(HelperTable[\"Sym\"], list(dfprices.columns))\n",
    "        HelperTable=HelperTable.sort_values(['Sym'])\n",
    "        ReturnsAtForecastEndDate.insert(0, end_date, HelperTable.iloc[:,:-1].values) \n",
    "    else:\n",
    "        ReturnsAtForecastEndDate=final.tail(1).iloc[:,-(len(symbolsWPortfolio))*3:].T\n",
    "        HelperTable=pd.concat([dfpricesFULL.tail(1).round(3).T]*(3)) \n",
    "        HelperTable[\"Sym\"]=HelperTable.index\n",
    "        HelperTable['Sym'] =pd.Categorical(HelperTable[\"Sym\"], list(dfprices.columns))\n",
    "        HelperTable=HelperTable.sort_values(['Sym'])\n",
    "        ReturnsAtForecastEndDate.insert(0, end_date, HelperTable.iloc[:,:-1].values)    \n",
    "           \n",
    "    ReturnsAtForecastEndDate[\"Returns Based On BStrp\"]=round((ReturnsAtForecastEndDate.iloc[:, 1]/ReturnsAtForecastEndDate.iloc[:, 0]-1)*100,2)\n",
    "    \n",
    "    return final, share_split_tableFULL , dfreturns , df_mean_stdev, ReturnsAtForecastEndDate, dfprices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalBSt, share_split , dfreturns , df_mean_stdev,ReturnsAtForecastEndDateBSt, dfprices =MonteCarlo_Bootstrap(start_date,end_date,backtest_duration,percentile_range,symbols,\\\n",
    "                       portfolioWeights,portfolioValue,T,N,NoOfIterationsMC,\"05678\",\"portfolioresults\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalBSt.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReturnsAtForecastEndDateBSt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([ReturnsAtForecastEndDateGBM,ReturnsAtForecastEndDateBSt],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cache(subfolder):\n",
    "    fileDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "    mydir = os.path.join(fileDir, f'static/{subfolder}')\n",
    "    filelist = [ f for f in os.listdir(mydir) if f.endswith(\".csv\") or f.endswith(\".png\") ]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(mydir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from https://intellipaat.com/community/34075/numpy-version-of-exponential-weighted-moving-average-equivalent-to-pandas-ewm-mean\n",
    "\n",
    "def numpy_ewma_vectorized_v2(data, window):\n",
    "    alpha = 2 /(window + 1.0)\n",
    "    alpha_rev = 1-alpha\n",
    "    n = data.shape[0]\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "    scale_arr = 1/pows[:-1]\n",
    "    offset = data[0]*pows[1:]\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out\n",
    "\n",
    "def movingaverageforecast(start_date,end_date,backtest_duration,symbols,portfolioWeights,portfolioValue,T,N,averagetype,windowsize,imagecounter,targetfolder):\n",
    "\n",
    "    dfprices, noOfShares, share_split_table = extract_prices(start_date,end_date,symbols,portfolioWeights,portfolioValue)\n",
    "    dfreturns ,df_mean_stdev=calc_returns(dfprices,symbols)\n",
    "\n",
    "    symbolsWPortfolio=symbols+[\"Portfolio\"]\n",
    "\n",
    "    resultantDF=[]\n",
    "    \n",
    "    if backtest_duration>0:\n",
    "        backtestdateslist=(list((dfprices.tail(backtest_duration).index)))\n",
    "    elif backtest_duration<=0:\n",
    "        backtestdateslist=(list((dfprices.tail(backtest_duration+1).index)))\n",
    "    backtestdates=[]\n",
    "    for i in backtestdateslist:\n",
    "        backtestdates.append(np.datetime64(datetime.strptime(str(i), '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "    extended_dates_future=[]\n",
    "    if backtest_duration>0:\n",
    "        for i in range(0,N-backtest_duration):\n",
    "            extended_dates_future.append(np.busday_offset(end_date, i, roll='forward'))\n",
    "    elif backtest_duration<=0:\n",
    "        for i in range(1,N-backtest_duration):\n",
    "            extended_dates_future.append(np.busday_offset(end_date, i, roll='forward'))\n",
    "        \n",
    "    extended_dates=backtestdates[0:len(backtestdates)-1]+extended_dates_future\n",
    "        \n",
    "    for i in range (0,len(dfreturns.columns)):\n",
    "        train=dfreturns.iloc[:,i][:len(dfreturns)-backtest_duration].values\n",
    "        dfpricestrain=dfprices.iloc[:,i][:len(dfreturns)-backtest_duration+1]\n",
    "\n",
    "        predictions = list()\n",
    "        history=train.tolist()\n",
    "\n",
    "        for j in range(0,T):\n",
    "        # make prediction\n",
    "            if averagetype==\"SMA\" :\n",
    "                yhat = mean(history[-windowsize:])\n",
    "            elif averagetype==\"EWMA\":\n",
    "                yhat=numpy_ewma_vectorized_v2(np.array(history), windowsize)[-1]\n",
    "            history.append(yhat)\n",
    "            predictions.append(yhat)\n",
    "\n",
    "        predictions=np.exp(predictions)\n",
    "        predictions=predictions.cumprod()*dfpricestrain.tail(1).values[0]\n",
    "        stocks=pd.DataFrame(predictions,index=extended_dates,columns=[f\"{averagetype} Forecast\"])\n",
    "        QQQ=pd.DataFrame(dfpricestrain.tail(1))\n",
    "        QQQ.columns=[f\"{averagetype} Forecast\"]\n",
    "        stocks=pd.concat([QQQ,stocks])\n",
    "\n",
    "        stocks=pd.concat([dfprices.iloc[:,i],stocks],axis=1)\n",
    "        stocks.tail(60).plot(figsize=(15,5))\n",
    "        plt.title(f\"{dfprices.columns[i]}: Forecast Via {averagetype} of {str(windowsize)} Days\")\n",
    "        plt.axvline(x=end_date,linestyle='dashed')\n",
    "        plt.savefig(f'static/{targetfolder}/{imagecounter}_movingaverage_{i}.png')\n",
    "        resultantDF.append(stocks)\n",
    "\n",
    "    if backtest_duration>0:\n",
    "        RMSE=[]\n",
    "        anothertable=[]\n",
    "\n",
    "        for i in range(0,len(dfprices.columns)):\n",
    "            temptable=resultantDF[i].tail(T).head(backtest_duration)\n",
    "            temptable[\"RMSE\"]=(temptable.iloc[:,0]-temptable.iloc[:,1])**2\n",
    "            RMSE.append((temptable.iloc[:,2].mean())**0.5)\n",
    "\n",
    "        RMSE_DF=pd.DataFrame(RMSE,index=dfprices.columns,columns=[\"RMSE For Backtest From \"+temptable.index[0].strftime(\"%Y-%m-%d\")+\" To \"+temptable.index[-1].strftime(\"%Y-%m-%d\")\\\n",
    "                                                               +\" (\"+str(len(temptable))+\" Days)\"])\n",
    "            \n",
    "    elif backtest_duration<=0:\n",
    "        RMSE_DF=pd.DataFrame()\n",
    "\n",
    "    return resultantDF, RMSE_DF, share_split_table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultantDF, RMSE_DF, share_split_table=movingaverageforecast(start_date,end_date,backtest_duration,symbols,portfolioWeights,portfolioValue,T,N,'SMA',26,\"123456\",\"movingaverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tailsDF=pd.DataFrame()\n",
    "for i in range(0,len(resultantDF)):\n",
    "    tailsDF=pd.concat([tailsDF,resultantDF[i].tail(20)],axis=1)\n",
    "tailsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from https://medium.com/analytics-vidhya/arima-garch-forecasting-with-python-7a3f797de3ff\n",
    "# Use this code snippet to customize the auto-arima\n",
    "# arima_model =  pmdarima.auto_arima(train,start_p=0, d=1, start_q=0, \n",
    "#                           max_p=5, max_d=5, max_q=5, seasonal=False, \n",
    "#                           error_action='warn',trace = True,\n",
    "#                           supress_warnings=True,stepwise = True,\n",
    "#                           random_state=20,n_fits = 50 )\n",
    "# Use for seasonal effects\n",
    "# seasonal=True,start_P=0,D=1, start_Q=0, max_P=0, max_D=0, max_Q=0, m=0,\n",
    "\n",
    "\n",
    "def UnivarArimaGarchPredict(start_date,end_date,backtest_duration,symbols,portfolioWeights,portfolioValue,T,N,imagecounter,targetfolder):\n",
    "    \n",
    "    dfprices, noOfShares, share_split_table = extract_prices(start_date,end_date,symbols,portfolioWeights,portfolioValue)\n",
    "    dfreturns ,df_mean_stdev=calc_returns(dfprices,symbols)\n",
    "    symbolsWPortfolio=symbols+[\"Portfolio\"]\n",
    "    backtest_end_date=str(np.busday_offset(end_date, -backtest_duration, roll='backward'))\n",
    "    extended_dates=[]\n",
    "    resultantDF=[]\n",
    "    \n",
    "    backtestdateslist=(list((dfprices.tail(backtest_duration).index)))\n",
    "    backtestdates=[]\n",
    "    for i in backtestdateslist:\n",
    "        backtestdates.append(np.datetime64(datetime.strptime(str(i), '%Y-%m-%d %H:%M:%S').strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "    extended_dates_future=[]\n",
    "    for i in range(0,N-backtest_duration):\n",
    "        extended_dates_future.append(np.busday_offset(end_date, i, roll='forward'))\n",
    "\n",
    "    extended_dates=backtestdates[0:len(backtestdates)-1]+extended_dates_future\n",
    "    \n",
    "    for i in range (0,len(dfreturns.columns)):\n",
    "    \n",
    "        returns=dfreturns.iloc[:,i][:len(dfreturns)-backtest_duration]*100 #*100 is for scaling purposes\n",
    "        dfpricestrain=dfprices.iloc[:,i][:len(dfprices)-backtest_duration] #Used later to extract last row prices\n",
    "    \n",
    "        # fit ARIMA on returns \n",
    "        arima_model = pmdarima.auto_arima(returns,trace = True)\n",
    "        p, d, q = arima_model.order\n",
    "        arimaaicvalue=round(arima_model.aic())\n",
    "        arima_residuals = arima_model.arima_res_.resid\n",
    "\n",
    "        # fit a GARCH(1,1) model on the residuals of the ARIMA model\n",
    "        garch = arch.arch_model(arima_residuals, p=1, q=1)\n",
    "        garch_fitted = garch.fit()\n",
    "        garchaicvalue=round(garch_fitted.aic)\n",
    "        print(garch_fitted.summary())\n",
    "\n",
    "        # Use ARIMA to predict mu mean term \n",
    "        # Use GARCH to predict the residual error term \n",
    "\n",
    "        predicted_mu = pd.DataFrame(arima_model.predict(n_periods=int(T)))\n",
    "\n",
    "        garch_forecast = garch_fitted.forecast(horizon=int(T))\n",
    "        predicted_et = garch_forecast.mean.iloc[-1:]\n",
    "        predicted_et=predicted_et.T\n",
    "\n",
    "        predictions=pd.DataFrame()\n",
    "        predictions[\"ARIMA predicted mu\"]=predicted_mu.iloc[:,0].values\n",
    "        predictions[\"GARCH 1,1, predicted et\"]=list(predicted_et.iloc[:,0])\n",
    "        predictions[\"ARIMA+GARCH\"]=predictions[\"ARIMA predicted mu\"]+predictions[\"GARCH 1,1, predicted et\"]\n",
    "        predictions=predictions/100\n",
    "\n",
    "        futurereturns=np.exp(predictions.iloc[:,2])\n",
    "        futurereturns=futurereturns.cumprod()\n",
    "        stocks=pd.DataFrame()\n",
    "        stocks[\"ARIMAGarch Forecast \"+symbolsWPortfolio[i]]=futurereturns*dfpricestrain.tail(1)[0]\n",
    "        \n",
    "        if backtest_duration>0:\n",
    "            stocks.index= extended_dates\n",
    "        elif backtest_duration<=0:\n",
    "            stocks.index= extended_dates[1:]\n",
    "        \n",
    "        QQQ=pd.DataFrame(dfpricestrain.tail(1))\n",
    "        QQQ.columns=[\"ARIMAGarch Forecast \"+symbolsWPortfolio[i]]\n",
    "        stocks=pd.concat([QQQ,stocks])\n",
    "\n",
    "        XYZ=pd.concat([dfprices.iloc[:,i],stocks], axis=1, sort=False)\n",
    "        resultantDF.append(XYZ)\n",
    "        XYZ.tail(60).plot(figsize=(15,5))\n",
    "        plt.title(f\"{symbolsWPortfolio[i]}: Forecast Via ARIMA ({str(p)},{str(q)},{str(d)})+ Constant Mean GARCH(1,1) with AIC {str(arimaaicvalue)} (ARIMA) and AIC {str(garchaicvalue)} (GARCH)\")\n",
    "        plt.axvline(x=end_date,linestyle='dashed')\n",
    "        plt.savefig(f'static/{targetfolder}/{imagecounter}_arimagarch{i}.png')\n",
    "        \n",
    "    if backtest_duration>0:\n",
    "        RMSE=[]\n",
    "        anothertable=[]\n",
    "\n",
    "        for i in range(0,len(dfprices.columns)):\n",
    "            temptable=resultantDF[i].tail(T).head(backtest_duration)\n",
    "            temptable[\"RMSE\"]=(temptable.iloc[:,0]-temptable.iloc[:,1])**2\n",
    "            RMSE.append((temptable.iloc[:,2].mean())**0.5)\n",
    "\n",
    "        RMSE_DF=pd.DataFrame(RMSE,index=dfprices.columns,columns=[\"RMSE For Backtest From \"+temptable.index[0].strftime(\"%Y-%m-%d\")+\" To \"+temptable.index[-1].strftime(\"%Y-%m-%d\")\\\n",
    "                                                               +\" (\"+str(len(temptable))+\" Days)\"])\n",
    "            \n",
    "    elif backtest_duration<=0:\n",
    "        RMSE_DF=pd.DataFrame()\n",
    "    \n",
    "    return resultantDF, RMSE_DF, share_split_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultantDF, RMSE_DF, share_split_table =UnivarArimaGarchPredict(start_date,end_date,backtest_duration,symbols,portfolioWeights,portfolioValue,T,N,'34567','arimagarch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultantDF[4].tail(150).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tailsDF=pd.DataFrame()\n",
    "for i in range(0,len(resultantDF)):\n",
    "    tailsDF=pd.concat([tailsDF,resultantDF[i].tail(20)],axis=1)\n",
    "tailsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "\n",
    "def seasonal_decomposition(start_date,end_date,symbols,seasonal_period,modeltype='multiplicative'):\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        dfprices = data.DataReader(symbols, start=start_date, end=end_date, data_source='yahoo')\n",
    "        dfprices = dfprices[['Adj Close']]\n",
    "    \n",
    "    dfprices.columns=[' '.join(col).strip() for col in dfprices.columns.values]\n",
    "    \n",
    "    for column in dfprices.columns:\n",
    "        series = dfprices[column]\n",
    "        decomposition = seasonal_decompose(series, model=modeltype, period = seasonal_period)\n",
    "\n",
    "        trend = decomposition.trend\n",
    "        seasonal = decomposition.seasonal\n",
    "        residual = decomposition.resid\n",
    "\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.suptitle(f\"Time Series Decomposition For {dfprices.columns[0]}\")\n",
    "        plt.subplot(411)\n",
    "        plt.plot(series, label='Adj Close', color=\"blue\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.subplot(412)\n",
    "        plt.plot(trend, label='Trend', color=\"blue\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.subplot(413)\n",
    "        plt.plot(seasonal,label='Seasonality', color=\"blue\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.subplot(414)\n",
    "        plt.plot(residual, label='Residuals', color=\"blue\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_decomposition(start_date,end_date,symbols,252,modeltype='additive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# https://medium.com/datadriveninvestor/how-to-build-exponential-smoothing-models-using-python-simple-exponential-smoothing-holt-and-da371189e1a1\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "\n",
    "\n",
    "def exp_smoothing_configs(trend=['add', 'mul', None],seasonal=['add','mul',None],seasonal_period=[None]):\n",
    "\tconfig = list()\n",
    "\t# define config lists\n",
    "\tt_params = trend\n",
    "\td_params = [True, False]\n",
    "\ts_params = seasonal\n",
    "\tp_params = seasonal_period\n",
    "\tb_params = [True, False]\n",
    "\tr_params = [True, False]\n",
    "\t# create config instances\n",
    "\tfor t in t_params:\n",
    "\t\tfor d in d_params:\n",
    "\t\t\tfor s in s_params:\n",
    "\t\t\t\tfor p in p_params:\n",
    "\t\t\t\t\tfor b in b_params:\n",
    "\t\t\t\t\t\tfor r in r_params:\n",
    "\t\t\t\t\t\t\tcfg = [t,d,s,p,b,r]\n",
    "\t\t\t\t\t\t\tconfig.append(cfg)\n",
    "\treturn config\n",
    "\n",
    "def AutoHoltWinters(start_date,end_date,forecast_end_date,backtestduration,symbols,portfolioWeights,portfolioValue,seasonalfreq,imagecounter,targetfolder):\n",
    "    \n",
    "    dfprices, noOfShares, share_split_table = extract_prices(start_date,end_date,symbols,portfolioWeights,portfolioValue)\n",
    "\n",
    "    configs=exp_smoothing_configs(trend=['add','mul'],seasonal=['add','mul',None],seasonal_period=[seasonalfreq])\n",
    "    \n",
    "    if backtestduration==0:\n",
    "        backtest_duration=1\n",
    "    elif backtestduration>0:\n",
    "        backtest_duration=backtestduration\n",
    "\n",
    "    T=np.busday_count(end_date,forecast_end_date)+backtest_duration\n",
    "    N=T+1\n",
    "    \n",
    "    OverallRMSEforConfigTable=[]\n",
    "    FinalOutput=[]\n",
    "    BestConfigSummary=[]\n",
    "    OptimalRMSESummaryList=[]\n",
    "    \n",
    "    for j in range(0,len(dfprices.columns)):\n",
    "        #split between the training and the test data sets. \n",
    "        df_test=dfprices.iloc[:,j][len(dfprices)-backtest_duration:]\n",
    "        df_train=dfprices.iloc[:,j][:len(dfprices)-backtest_duration]\n",
    "        \n",
    "        extended_dates_future=[]\n",
    "        for k in range(0,N-backtest_duration):\n",
    "            extended_dates_future.append(pd.Timestamp(numpy.datetime64(np.busday_offset(end_date, k, roll='forward'))))\n",
    "\n",
    "        extended_dates=df_test.index.tolist() +extended_dates_future[1:]\n",
    "\n",
    "        RMSEforConfig=[]\n",
    "        counter=0\n",
    "\n",
    "        for i in range(0,len(configs)):\n",
    "\n",
    "            try:\n",
    "                #build and train the model on the training data\n",
    "                #TESmodel = ExponentialSmoothing(df_train, trend=t, damped_trend=d, seasonal=s, seasonal_periods=p)\n",
    "                TESmodel = ExponentialSmoothing(df_train, trend=configs[i][0], damped_trend=configs[i][1], seasonal=configs[i][2], seasonal_periods=configs[i][3])\n",
    "\n",
    "                #TESmodel_fit = TESmodel.fit(optimized=True, use_boxcox=b, remove_bias=r)\n",
    "                TESmodel_fit = TESmodel.fit(optimized=True, use_boxcox=configs[i][4], remove_bias=configs[i][5])\n",
    "\n",
    "                forecastpast = TESmodel_fit.predict(0)\n",
    "                forecastfuture =  TESmodel_fit.forecast(len(df_test))\n",
    "                forecast=np.concatenate((forecastpast.values,forecastfuture.values))\n",
    "                forecastlist=list(forecast)\n",
    "\n",
    "                df_forecast_TES=pd.DataFrame(forecastlist,index=dfprices.index,columns=[\"TES Forecast\"])\n",
    "                TESoutput=pd.concat([dfprices.iloc[:,j],df_forecast_TES],axis=1)\n",
    "\n",
    "                RMSEtraincalctable=TESoutput.head(len(df_train))\n",
    "\n",
    "                RMSEtraincalctable[\"Sq Diff\"]=(RMSEtraincalctable[RMSEtraincalctable.columns[0]]-RMSEtraincalctable[RMSEtraincalctable.columns[1]])**2\n",
    "                RMSEtrain=(RMSEtraincalctable[\"Sq Diff\"].mean())**0.5\n",
    "\n",
    "#                 RMSEtestcalctable=TESoutput.tail(len(extended_dates))\n",
    "#                 RMSEtestcalctable=RMSEtestcalctable.head(len(df_test))\n",
    "\n",
    "#                 RMSEtestcalctable[\"Sq Diff\"]=(RMSEtestcalctable[RMSEtestcalctable.columns[0]]-RMSEtestcalctable[RMSEtestcalctable.columns[1]])**2\n",
    "#                 RMSEtest=(RMSEtestcalctable[\"Sq Diff\"].mean())**0.5\n",
    "                \n",
    "                RMSEforConfig.append([counter,configs[i][0],configs[i][1],configs[i][2],configs[i][3],configs[i][4],configs[i][5],RMSEtrain])\n",
    "                counter=counter+1\n",
    "            #     print(f'Test Data RMSE over {str(len(df_test))} days: {str(RMSEtest)}')\n",
    "            except ValueError:\n",
    "                RMSEforConfig.append([counter,configs[i][0],configs[i][1],configs[i][2],configs[i][3],configs[i][4],configs[i][5],'Error'])\n",
    "                counter=counter+1\n",
    "\n",
    "        RMSEforConfigTable=pd.DataFrame(RMSEforConfig,columns=['No','trend t','damped trend d','seasonal s','seasonal periods p','use boxcox b','remove bias r','RMSE For Train Data'])\n",
    "        \n",
    "        OverallRMSEforConfigTable.append(RMSEforConfigTable)\n",
    "        \n",
    "        BestConfig=RMSEforConfigTable[RMSEforConfigTable['RMSE For Train Data']!='Error']\n",
    "        BestConfig=BestConfig.dropna()\n",
    "        BestConfig=BestConfig[BestConfig['RMSE For Train Data']==BestConfig['RMSE For Train Data'].min()]\n",
    "        \n",
    "        OptimalTESmodel = ExponentialSmoothing(df_train, trend=BestConfig.iloc[0]['trend t'], damped_trend=BestConfig.iloc[0]['damped trend d']\\\n",
    "                                        , seasonal=BestConfig.iloc[0]['seasonal s'], seasonal_periods=BestConfig.iloc[0]['seasonal periods p'])\n",
    "\n",
    "        OptimalTESmodel_fit = OptimalTESmodel.fit(optimized=True, use_boxcox=BestConfig.iloc[0]['use boxcox b'], remove_bias=BestConfig.iloc[0]['remove bias r'])\n",
    "\n",
    "        Optimalforecastpast = OptimalTESmodel_fit.predict(0)\n",
    "        Optimalforecastfuture =  OptimalTESmodel_fit.forecast(len(extended_dates))\n",
    "        Optimalforecast=np.concatenate((Optimalforecastpast.values,Optimalforecastfuture.values))\n",
    "        Optimalforecastlist=list(Optimalforecast)\n",
    "        \n",
    "\n",
    "        Optimaldf_forecast_TES=pd.DataFrame(Optimalforecastlist,index=df_train.index.tolist()+extended_dates,columns=[\"TES Forecast\"])\n",
    "        OptimalTESoutput=pd.concat([dfprices.iloc[:,j],Optimaldf_forecast_TES],axis=1)\n",
    "\n",
    "        OptimalRMSEtraincalctable=OptimalTESoutput.head(len(df_train))\n",
    "\n",
    "        OptimalRMSEtraincalctable[\"Sq Diff\"]=(OptimalRMSEtraincalctable[OptimalRMSEtraincalctable.columns[0]]-OptimalRMSEtraincalctable[OptimalRMSEtraincalctable.columns[1]])**2\n",
    "        OptimalRMSEtrain=(OptimalRMSEtraincalctable[\"Sq Diff\"].mean())**0.5\n",
    "\n",
    "        OptimalRMSEtestcalctable=OptimalTESoutput.tail(len(extended_dates))\n",
    "        OptimalRMSEtestcalctable=OptimalRMSEtestcalctable.head(len(df_test))\n",
    "\n",
    "        OptimalRMSEtestcalctable[\"Sq Diff\"]=(OptimalRMSEtestcalctable[OptimalRMSEtestcalctable.columns[0]]-OptimalRMSEtestcalctable[OptimalRMSEtestcalctable.columns[1]])**2\n",
    "        OptimalRMSEtest=(OptimalRMSEtestcalctable[\"Sq Diff\"].mean())**0.5\n",
    "\n",
    "        OptimalTESoutput.tail(60).plot(figsize=(15,5))\n",
    "        titlestring=f\"{dfprices.columns[j]}: Holt Winters Best Fit= trend:{str(BestConfig.iloc[0]['trend t'])}, damped trend:{str(BestConfig.iloc[0]['damped trend d'])}, seasonal s:{str(BestConfig.iloc[0]['seasonal s'])}, seasonal periods:{str(BestConfig.iloc[0]['seasonal periods p'])}, use boxcox:{str(BestConfig.iloc[0]['use boxcox b'])}, remove bias:{str(BestConfig.iloc[0]['remove bias r'])}\"\n",
    "        titlestring=str(titlestring)\n",
    "        plt.title(titlestring)\n",
    "        if backtestduration>0:\n",
    "            print(\"RMSE-Train Data:\"+str(round(OptimalRMSEtrain,2))+\" RMSE-Test Data From \"+OptimalRMSEtestcalctable.index[0].strftime(\"%Y-%m-%d\")+\\\n",
    "                                          \" To \"+OptimalRMSEtestcalctable.index[-1].strftime(\"%Y-%m-%d\")+\" (\"+str(len(OptimalRMSEtestcalctable))+\\\n",
    "                                          \" Days) :\"+str(round(OptimalRMSEtest,2)))\n",
    "        if backtestduration==0:\n",
    "            print(\"RMSE-Train Data:\"+str(round(OptimalRMSEtrain,2)))\n",
    "            \n",
    "        plt.axvline(x=end_date,linestyle='dashed')\n",
    "        #plt.axvline(x=df_test.index[0],linestyle='dashed')\n",
    "        plt.savefig(f'static/{targetfolder}/{imagecounter}_{j}_TESForecast.png')\n",
    "        \n",
    "        FinalOutput.append(OptimalTESoutput)\n",
    "        BestConfigSummary.append(BestConfig)\n",
    "\n",
    "        OptimalRMSESummaryList.append(OptimalRMSEtest)\n",
    "        \n",
    "    if backtestduration>0:\n",
    "        OptimalRMSESummaryTable=pd.DataFrame([OptimalRMSESummaryList])\n",
    "        OptimalRMSESummaryTable=OptimalRMSESummaryTable.T\n",
    "        OptimalRMSESummaryTable.columns=[\"RMSE-Test Data From \"+OptimalRMSEtestcalctable.index[0].strftime(\"%Y-%m-%d\")+\\\n",
    "                                            \" To \"+OptimalRMSEtestcalctable.index[-1].strftime(\"%Y-%m-%d\")+\" (\"+str(len(OptimalRMSEtestcalctable))+\\\n",
    "                                            \" Days)\"]\n",
    "        OptimalRMSESummaryTable[\"Stocks\"]=dfprices.columns\n",
    "        OptimalRMSESummaryTable=OptimalRMSESummaryTable[[\"Stocks\",\"RMSE-Test Data From \"+OptimalRMSEtestcalctable.index[0].strftime(\"%Y-%m-%d\")+\\\n",
    "                                            \" To \"+OptimalRMSEtestcalctable.index[-1].strftime(\"%Y-%m-%d\")+\" (\"+str(len(OptimalRMSEtestcalctable))+\\\n",
    "                                            \" Days)\"]]\n",
    "    elif backtestduration==0:\n",
    "        OptimalRMSESummaryTable=pd.DataFrame()\n",
    "        \n",
    "        \n",
    "    BestConfigSummaryTable=pd.DataFrame()\n",
    "    for l in range(0,len(dfprices.columns)):\n",
    "        BestConfigSummaryTable=pd.concat([BestConfigSummaryTable,BestConfigSummary[l]])\n",
    "\n",
    "    BestConfigSummaryTable[\"Stocks\"]=dfprices.columns\n",
    "    BestConfigSummaryTable=BestConfigSummaryTable[['Stocks','trend t','damped trend d','seasonal s','seasonal periods p','use boxcox b','remove bias r','RMSE For Train Data']]\n",
    "        \n",
    "    FinalOutputTailTable=pd.DataFrame()\n",
    "\n",
    "    for m in range(0,len(dfprices.columns)):\n",
    "        FinalOutputTailTable=pd.concat([FinalOutputTailTable,FinalOutput[m].tail(60)],axis=1)\n",
    "\n",
    "    FinalOutputTailTable\n",
    "\n",
    "    return FinalOutput,FinalOutputTailTable,BestConfigSummaryTable,OptimalRMSESummaryTable,OverallRMSEforConfigTable       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# FinalOutput,FinalOutputTailTable,BestConfigSummaryTable,OptimalRMSESummaryTable,OverallRMSEforConfigTable=AutoHoltWinters(start_date,end_date,forecast_end_date,backtest_duration,symbols,portfolioWeights,portfolioValue,252,'345456','holtwinters')\n",
    "\n",
    "FinalOutput,FinalOutputTailTable,BestConfigSummaryTable,OptimalRMSESummaryTable,OverallRMSEforConfigTable  =AutoHoltWinters(start_date,end_date,forecast_end_date,backtest_duration,symbols,portfolioWeights,portfolioValue,5,'345456','holtwinters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH 253 DAYS\n",
    "\n",
    "BestConfigSummaryTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH 5 DAYS\n",
    "\n",
    "BestConfigSummaryTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STANDALONE - TEST \n",
    "'''\n",
    "df_test=dfprices.iloc[:,1][len(dfprices)-backtest_duration:]\n",
    "df_train=dfprices.iloc[:,1][:len(dfprices)-backtest_duration]\n",
    "        \n",
    "T=np.busday_count(end_date,forecast_end_date)+backtest_duration\n",
    "N=T+1\n",
    "        \n",
    "extended_dates_future=[]\n",
    "for i in range(0,N-backtest_duration):\n",
    "    extended_dates_future.append(pd.Timestamp(numpy.datetime64(np.busday_offset(end_date, i, roll='forward'))))\n",
    "\n",
    "extended_dates=df_test.index.tolist() +extended_dates_future[1:]\n",
    "\n",
    "xconfigs=['add',False,'add',252,False,True]\n",
    "\n",
    "#build and train the model on the training data\n",
    "#TESmodel = ExponentialSmoothing(df_train, trend=t, damped_trend=d, seasonal=s, seasonal_periods=p)\n",
    "TESmodel = ExponentialSmoothing(df_train, trend=xconfigs[0], damped_trend=xconfigs[1], seasonal=xconfigs[2], seasonal_periods=xconfigs[3])\n",
    "\n",
    "#TESmodel_fit = TESmodel.fit(optimized=True, use_boxcox=b, remove_bias=r)\n",
    "TESmodel_fit = TESmodel.fit(optimized=True, use_boxcox=xconfigs[4], remove_bias=xconfigs[5])\n",
    "\n",
    "forecastpast = TESmodel_fit.predict(0)\n",
    "forecastfuture =  TESmodel_fit.forecast(len(extended_dates))\n",
    "forecast=np.concatenate((forecastpast.values,forecastfuture.values))\n",
    "forecastlist=list(forecast)\n",
    "\n",
    "df_forecast_DES=pd.DataFrame(forecastlist,index=df_train.index.tolist()+extended_dates,columns=[\"DES Forecast\"])\n",
    "df_forecast_DES\n",
    "DESoutput=pd.concat([dfprices.iloc[:,1],df_forecast_DES],axis=1)\n",
    "\n",
    "RMSEtraincalctable=DESoutput.head(len(df_train))\n",
    "\n",
    "RMSEtraincalctable[\"Sq Diff\"]=(RMSEtraincalctable[RMSEtraincalctable.columns[0]]-RMSEtraincalctable[RMSEtraincalctable.columns[1]])**2\n",
    "RMSEtrain=(RMSEtraincalctable[\"Sq Diff\"].mean())**0.5\n",
    "\n",
    "RMSEtestcalctable=DESoutput.tail(len(extended_dates))\n",
    "RMSEtestcalctable=RMSEtestcalctable.head(len(df_test))\n",
    "\n",
    "RMSEtestcalctable[\"Sq Diff\"]=(RMSEtestcalctable[RMSEtestcalctable.columns[0]]-RMSEtestcalctable[RMSEtestcalctable.columns[1]])**2\n",
    "RMSEtest=(RMSEtestcalctable[\"Sq Diff\"].mean())**0.5\n",
    "\n",
    "DESoutput.plot(figsize=(15,5))\n",
    "if backtest_duration>0:\n",
    "    print(\"RMSE-Train Data:\"+str(round(RMSEtrain,4))+\" RMSE-Test Data From \"+RMSEtestcalctable.index[0].strftime(\"%Y-%m-%d\")+\\\n",
    "                                  \" To \"+RMSEtestcalctable.index[-1].strftime(\"%Y-%m-%d\")+\" (\"+str(len(RMSEtestcalctable))+\\\n",
    "                                  \" Days) :\"+str(round(RMSEtest,4)))\n",
    "\n",
    "elif backtest_duration==0:\n",
    "    print(\"RMSE-Train Data:\"+str(round(RMSEtrain,4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EfficientPortfolio(start_date,end_date,symbols,portfolioValue,T,N,NoOfIterationsMC,AnnualRiskFreeRate,SimMethod,imagecounter,targetfolder):\n",
    "    \n",
    "    RiskFreeRate=(1+AnnualRiskFreeRate)**(1/252)-1\n",
    "    #Effective rate for period = (1 + annual rate)**(1 / # of periods) – 1\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        dfprices = data.DataReader(symbols, start=start_date, end=end_date, data_source='yahoo')\n",
    "        dfprices = dfprices[['Adj Close']]\n",
    "    dfprices.columns=[' '.join(col).strip() for col in dfprices.columns.values]\n",
    "    \n",
    "    priceAtEndDate=[]\n",
    "    for symbol in symbols:\n",
    "        priceAtEndDate.append(dfprices[[f'Adj Close {symbol}']][-(1):].values[0][0])\n",
    "    \n",
    "    symbolsWPortfolio=symbols+[\"Portfolio\"]\n",
    "    \n",
    "    ResultsTable=[]\n",
    "    \n",
    "    for i in range(0,NoOfIterationsMC):\n",
    "        \n",
    "        dfprices_inner=dfprices\n",
    "        portfolioWeightsRandom=list(np.random.dirichlet(np.ones(len(symbols)),size=1)[0])\n",
    "        \n",
    "        noOfShares=[]\n",
    "        portfolioValPerSymbol=[x * portfolioValue for x in portfolioWeightsRandom]\n",
    "        for j in range(0,len(symbols)):\n",
    "            noOfShares.append(portfolioValPerSymbol[j]/priceAtEndDate[j])\n",
    "        noOfShares=[round(element, 5) for element in noOfShares]\n",
    "        listOfColumns=dfprices_inner.columns.tolist()   \n",
    "        dfprices_inner[\"Adj Close Portfolio\"]=dfprices_inner[listOfColumns].mul(noOfShares).sum(1)\n",
    "\n",
    "        dfreturns ,df_mean_stdev=calc_returns(dfprices_inner,symbols)\n",
    "        \n",
    "        S0=np.array(dfprices.tail(1).values.tolist()[0])\n",
    "        mu=np.array(df_mean_stdev[\"Mean Log Daily Return\"].values.tolist())\n",
    "        sigma=np.array(df_mean_stdev[\"StdDev Log Daily Return\"].values.tolist())\n",
    "        \n",
    "        if SimMethod==\"GBM\":\n",
    "            if len(symbols)==1:\n",
    "                stocks, time = GBMsimulatorUniVar(S0, mu, sigma, T, N)\n",
    "                prediction=pd.DataFrame(stocks)\n",
    "                prediction=prediction.T\n",
    "                prediction.columns=dfprices.columns\n",
    "\n",
    "            else:\n",
    "                Cov=create_covar(dfreturns)\n",
    "                stocks, time = GBMsimulatorMultiVar(S0, mu, sigma, Cov, T, N)\n",
    "                prediction=pd.DataFrame(stocks)\n",
    "                prediction=prediction.T\n",
    "                prediction.columns=dfprices.columns\n",
    "     \n",
    "            IterationReturn,Iteration_Mean_Stdev=calc_returns(prediction,symbols)\n",
    "            IterationStdDev=Iteration_Mean_Stdev.tail(1).values[0][2]\n",
    "            IterationMean=Iteration_Mean_Stdev.tail(1).values[0][1]\n",
    "            \n",
    "            negativereturnsonly=pd.DataFrame(IterationReturn.iloc[:,len(IterationReturn.columns)-1])\n",
    "            negativereturnsonly=negativereturnsonly[negativereturnsonly['Log Daily Returns Adj Close Portfolio']<0]\n",
    "            IterationNegativeReturnsStdDev=negativereturnsonly['Log Daily Returns Adj Close Portfolio'].std()\n",
    "        \n",
    "        elif SimMethod==\"Bootstrap\":\n",
    "            \n",
    "            prediction=bootstrapforecast(dfreturns,T)\n",
    "            IterationStdDev=prediction.iloc[:,0].std()\n",
    "            IterationMean=prediction.iloc[:,0].mean()\n",
    "            negativereturnsonly=prediction[prediction['Log Daily Returns Adj Close Portfolio']<0].iloc[:,0]\n",
    "            IterationNegativeReturnsStdDev=negativereturnsonly.std()\n",
    "        \n",
    "        # Note to go from LOG returns to Simple returns , I used simple returns =exp(log returns)−1 \n",
    "        IterationSharpeRatio=round(((np.exp(IterationMean)-1)-RiskFreeRate)/(np.exp(IterationStdDev)-1),3)\n",
    "        \n",
    "        IterationSortinoRatio=round(((np.exp(IterationMean)-1)-RiskFreeRate)/(np.exp(IterationNegativeReturnsStdDev)-1),3)\n",
    "        \n",
    "        X=[portfolioWeightsRandom,IterationStdDev,IterationMean,IterationSharpeRatio,IterationSortinoRatio]\n",
    "        \n",
    "        ResultsTable.append(X)\n",
    "        \n",
    "        dfprices_inner.drop('Adj Close Portfolio',inplace=True, axis=1)\n",
    "    \n",
    "    FinalResultsTable=pd.DataFrame(ResultsTable,columns=[\"Weights\",\"Std Dev\",\"Mean\",\"Sharpe Ratio\",\"Sortino Ratio\"])\n",
    "    \n",
    "    historical_dfreturns ,historical_df_mean_stdev=calc_returns(dfprices,symbols)\n",
    "    \n",
    "    historical_df_mean_stdev=historical_df_mean_stdev[['Stock','StdDev Log Daily Return','Mean Log Daily Return']]\n",
    "    historical_df_mean_stdev.columns=['Stock','Std Dev','Mean']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    FinalResultsTable.plot.scatter(x=\"Std Dev\",y='Mean',ax=ax)\n",
    "    historical_df_mean_stdev.plot.scatter(x=\"Std Dev\",y='Mean',c='r',marker='x',ax=ax)\n",
    "\n",
    "    SharpeStdDev=FinalResultsTable.nlargest(1,['Sharpe Ratio'])['Std Dev'].values[0]\n",
    "    SharpeMean=FinalResultsTable.nlargest(1,['Sharpe Ratio'])['Mean'].values[0]\n",
    "    Sharperoundedweights=[round(num, 4) for num in FinalResultsTable.nlargest(1,['Sharpe Ratio'])['Weights'].values[0]]\n",
    "    Sharpeweightstring=[]\n",
    "    for i in range(0,len(symbols)):\n",
    "        Sharpeweightstring.append([symbols[i]+\":\",Sharperoundedweights[i]])\n",
    "    SharpeLabel=\"Optimal Sharpe Ratio\"\n",
    "    SharpeDetail='Optimal Sharpe Ratio: '+str(FinalResultsTable.nlargest(1,['Sharpe Ratio'])['Sharpe Ratio'].values[0])+\" with Weights \"+str(Sharpeweightstring)\n",
    "\n",
    "    SortinoStdDev=FinalResultsTable.nlargest(1,['Sortino Ratio'])['Std Dev'].values[0]\n",
    "    SortinoMean=FinalResultsTable.nlargest(1,['Sortino Ratio'])['Mean'].values[0]\n",
    "    Sortinoroundedweights=[round(num, 4) for num in FinalResultsTable.nlargest(1,['Sortino Ratio'])['Weights'].values[0]]\n",
    "    Sortinoweightstring=[]\n",
    "    for i in range(0,len(symbols)):\n",
    "        Sortinoweightstring.append([symbols[i]+\":\",Sortinoroundedweights[i]])\n",
    "    SortinoLabel='Optimal Sortino Ratio'\n",
    "    SortinoDetail='Optimal Sortino Ratio: '+str(FinalResultsTable.nlargest(1,['Sortino Ratio'])['Sortino Ratio'].values[0])+\" with Weights \"+str(Sortinoweightstring)\n",
    "    \n",
    "    SharpeSortino=pd.DataFrame(zip([SharpeStdDev,SortinoStdDev],[SharpeMean,SortinoMean]),index=['Optimal Sharpe','Optimal Sortino'],columns=['Std Dev','Mean'])\n",
    "    SharpeSortino.plot.scatter(x=\"Std Dev\",y='Mean',c='g',marker='x',ax=ax)\n",
    "    \n",
    "    txt=list(historical_df_mean_stdev['Stock'])+[SharpeLabel,SortinoLabel]\n",
    "    z=list(historical_df_mean_stdev['Std Dev'])+[SharpeStdDev,SortinoStdDev]\n",
    "    y=list(historical_df_mean_stdev['Mean'])+[SharpeMean,SortinoMean]\n",
    "\n",
    "    for i, text in enumerate(txt):\n",
    "        ax.annotate(text, (z[i], y[i]))\n",
    "    \n",
    "    plt.savefig(f'static/{targetfolder}/{imagecounter}_efficientportfolio.png')\n",
    "    print(SharpeDetail)\n",
    "    print(SortinoDetail)\n",
    "    \n",
    "    FinalResultsTable['Log Returns Std Dev']=FinalResultsTable['Std Dev']\n",
    "    FinalResultsTable['Log Returns Mean']=FinalResultsTable['Mean']\n",
    "    FinalResultsTable=FinalResultsTable[['Weights','Log Returns Std Dev','Log Returns Mean','Sharpe Ratio','Sortino Ratio']]\n",
    "    \n",
    "    return FinalResultsTable, SharpeDetail, SortinoDetail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a=EfficientPortfolio(start_date,end_date,symbols,portfolioValue,150,151,20,0.12,\"GBM\",\"12323\",\"efficientportfolio\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.nlargest(1,['Sharpe Ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SharpeWeights=a.nlargest(1,['Sharpe Ratio'])['Weights'].tolist()\n",
    "plotpiechart(symbols,SharpeWeights[0],\"243456\",\"efficientportfolio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VECTOR AUTOREGRESSION\n",
    "\n",
    "#https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/\n",
    "#https://www.machinelearningplus.com/time-series/vector-autoregression-examples-python/\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "\n",
    "# Cointegration test helps to establish the presence of a statistically \n",
    "# significant connection between two or more time series.  \n",
    "def cointegration_test(df,LagOrder_LowestAIC,alpha=0.05): \n",
    "    \"\"\"Perform Johanson's Cointegration Test and Report Summary\"\"\"\n",
    "    out = coint_johansen(df,-1,LagOrder_LowestAIC-1)\n",
    "#     coint_johansen(df,det_order,Number of lagged differences in the model)    \n",
    "#     det_order = order of time polynomial in the null-hypothesis\n",
    "#     det_order = -1, no deterministic part\n",
    "#     det_order =  0, for constant term\n",
    "#     det_order =  1, for constant plus time-trend  \n",
    "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
    "    traces = out.lr1\n",
    "    cvts = out.cvt[:, d[str(1-alpha)]]\n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    cointDFarray=[]\n",
    "    for col, trace, cvt in zip(df.columns, traces, cvts):\n",
    "        cointDFarray.append([col,round(trace,2),cvt, trace>cvt])\n",
    "    cointDF=pd.DataFrame(cointDFarray,columns=[\"Name\",\"Johanson Cointegration Test Stat\",\"Critical Value 5% Signif Lvl\",\"Accept(i.e True) / Reject(i.e False)\"])\n",
    "    cointDF=cointDF[[\"Name\",\"Johanson Cointegration Test Stat\",\"Critical Value 5% Signif Lvl\",\"Accept(i.e True) / Reject(i.e False)\"]]\n",
    "    \n",
    "    return cointDF\n",
    "\n",
    "\n",
    "# Granger’s causality tests the null hypothesis that the coefficients of past values in the regression equation is zero.\n",
    "# In simpler terms, the past values of time series (X) do not cause the other series (Y). \n",
    "# So, if the p-value obtained from the test is lesser than the significance level of 0.05, \n",
    "# then, you can safely reject the null hypothesis.\n",
    "\n",
    "maxlag=21 #Reflects a month of lag if 1 year ~252 trading days\n",
    "test = 'ssr_chi2test'\n",
    "\n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
    "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "    The rows are the response variable, columns are predictors. The values in the table \n",
    "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "    zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "    data      : pandas dataframe containing the time series variables\n",
    "    variables : list containing names of the time series variables.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "\n",
    "\n",
    "def vectorAutoRegression(start_date,end_date,symbols,portfolioWeights,portfolioValue,forecast_end_date,backtest_duration,imagecounter,targetfolder):\n",
    "    \n",
    "    if len(symbols)>1:\n",
    "        dfprices, noOfShares, share_split_table = extract_prices(start_date,end_date,symbols,portfolioWeights,portfolioValue)\n",
    "        dfreturns ,df_mean_stdev=calc_returns(dfprices,symbols)\n",
    "\n",
    "        T=np.busday_count(end_date,forecast_end_date)+backtest_duration\n",
    "        N=T+1\n",
    "\n",
    "        #creating the train and validation set\n",
    "        symbolsWPortfolio=symbols+[\"Portfolio\"]\n",
    "        train = dfreturns[:len(dfreturns)-backtest_duration]\n",
    "        pricetrain = dfprices[:len(dfreturns)-backtest_duration+1]\n",
    "        valid = dfreturns[len(dfreturns)-backtest_duration:]\n",
    "        cols=train.columns\n",
    "\n",
    "        #fit the model\n",
    "\n",
    "        VARmodel = VAR(endog=train)\n",
    "\n",
    "        bestAIC=[]\n",
    "        for i in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]: #reflects test for lag order up till 1 mth ~21 days/mth\n",
    "            result = VARmodel.fit(i)\n",
    "            bestAIC.append([i, result.aic])\n",
    "        bestAIC_DF=pd.DataFrame(bestAIC,columns=[\"Lag Order\",\"AIC\"])    \n",
    "        LagOrder_LowestAIC=bestAIC_DF.nsmallest(1,['AIC'])['Lag Order'].values[0]\n",
    "        \n",
    "        #In a VAR(p) model, the first p lags of each variable in the system is used as regression predictors for EACH variable\n",
    "\n",
    "        VARmodel_fit = VARmodel.fit(LagOrder_LowestAIC)\n",
    "        VARmodel_fit.summary()\n",
    "\n",
    "        from statsmodels.stats.stattools import durbin_watson\n",
    "        out = durbin_watson(VARmodel_fit.resid)\n",
    "\n",
    "        DWTableElements=[]\n",
    "        for col, val in zip(cols, out):\n",
    "            DWTableElements.append([col,val])\n",
    "\n",
    "        DWTable=pd.DataFrame(DWTableElements,columns=[\"Stock Log Returns\",\"Serial Corr\"])\n",
    "\n",
    "        extended_dates_future=[]\n",
    "        for i in range(0,N-backtest_duration):\n",
    "            extended_dates_future.append(pd.Timestamp(numpy.datetime64(np.busday_offset(end_date, i, roll='forward'))))\n",
    "\n",
    "        extended_dates=valid.index.tolist() +extended_dates_future[1:]\n",
    "\n",
    "        # make prediction on validation\n",
    "        prediction = VARmodel_fit.forecast(VARmodel_fit.y, steps=len(extended_dates))\n",
    "\n",
    "        #converting predictions to dataframe\n",
    "        pred = pd.DataFrame(prediction,index=extended_dates,columns=[cols])\n",
    "\n",
    "        pred\n",
    "        startingrefprice=pricetrain.tail(1)\n",
    "\n",
    "        stockprices=np.exp(pred)\n",
    "        stockprices=stockprices.cumprod()\n",
    "        stockprices=stockprices.mul(startingrefprice.values[0])\n",
    "        stockprices.columns=dfprices.columns\n",
    "        firstrow=pd.DataFrame(startingrefprice)\n",
    "        stockprices=pd.concat([firstrow,stockprices])\n",
    "        stockprices=stockprices.add_prefix(\"VAR_Forecast \")\n",
    "        actual=dfprices\n",
    "        stockprices=pd.concat([actual,stockprices],axis=1)\n",
    "        for symbol in symbolsWPortfolio:\n",
    "            stockprices[[\"Adj Close \"+symbol,\"VAR_Forecast Adj Close \"+symbol]].tail(60).plot(figsize=(15,5))\n",
    "            plt.title(symbol+\": Vector Auto Regression Forecast With Optimal Lag Order=\"+str(bestAIC_DF.nsmallest(1,[\"AIC\"])[\"Lag Order\"].values[0])\\\n",
    "                     +\" (AIC Value:\"+str(bestAIC_DF.nsmallest(1,[\"AIC\"])[\"AIC\"].values[0])+\" )\")\n",
    "            plt.axvline(x=end_date,linestyle='dashed')\n",
    "            plt.savefig(f'static/{targetfolder}/{imagecounter}_{symbol}VectorAR.png')\n",
    "            \n",
    "\n",
    "\n",
    "        RMSECalcTable=stockprices.tail(len(extended_dates))\n",
    "        RMSECalcTable=RMSECalcTable.head(len(valid))\n",
    "\n",
    "        if backtest_duration>0:\n",
    "            RMSE=[]\n",
    "            anothertable=[]\n",
    "\n",
    "            for i in range(0,len(dfprices.columns)):\n",
    "                temptable=RMSECalcTable\n",
    "                temptable[\"RMSE\"]=(temptable.iloc[:,i]-temptable.iloc[:,i+len(dfprices.columns)])**2\n",
    "                RMSE.append((temptable[\"RMSE\"].mean())**0.5)\n",
    "\n",
    "            RMSE_DF=pd.DataFrame(RMSE,index=dfprices.columns,columns=[\"RMSE For Backtest From \"+temptable.index[0].strftime(\"%Y-%m-%d\")+\" To \"+temptable.index[-1].strftime(\"%Y-%m-%d\")\\\n",
    "                                                                       +\" (\"+str(len(temptable))+\" Days)\"])\n",
    "\n",
    "        elif backtest_duration<=0:\n",
    "            RMSE_DF=pd.DataFrame()\n",
    "\n",
    "        print(\"Root Mean Square Error For Backtest Period\")    \n",
    "        display(RMSE_DF)\n",
    "\n",
    "        print('DURBIN WATSON TEST')\n",
    "        print('Checks the serial correlation of the residuals errors if there is some pattern in the time series not explained by the Vector AutoRegression model')\n",
    "        print('#Statistic ranges between 0 to 4. The closer to 2, then no significant serial correlation.The closer to 0, there is a positive serial correlation, closer to 4 implies negative serial correlation.')\n",
    "        display(DWTable)\n",
    "\n",
    "        GrangersMatrix=grangers_causation_matrix(dfreturns, variables = dfreturns.columns)\n",
    "        print('GRANGER CAUSALITY TEST (Using Sum Of Squared Residuals With Chi Square Test For Max Lag Of 1 Month (21 Trading Days)')\n",
    "        print('Rows are the Response _y and columns are the predictor series _x so values refers to the p-value of log returns stock A_x causing log returns stock B_y')\n",
    "        print('If the p-value obtained from the test is lesser than the significance level of 0.05, then, you can safely reject the null hypothesis that that the coefficients of past values in the regression equation is zero')\n",
    "        display(GrangersMatrix)\n",
    "\n",
    "        JohansonCointTable=cointegration_test(dfreturns,LagOrder_LowestAIC)\n",
    "        print('JOHANSON COINTEGRATION TEST (Assuming no deterministic part for the order of time polynomial)') \n",
    "        print('Establishes the presence of a statistically significant connection between two or more time series.')\n",
    "        print('If Test Stat > Critical Value (TRUE) then we accept that there is a significant connection')\n",
    "        display(JohansonCointTable)\n",
    "    \n",
    "    if len(symbols)==1:\n",
    "        stockprices=pd.DataFrame()\n",
    "        RMSE_DF=pd.DataFrame()\n",
    "        DWTable=pd.DataFrame()\n",
    "        GrangersMatrix=pd.DataFrame()\n",
    "        JohansonCointTable=pd.DataFrame()\n",
    "        \n",
    "        print('Error-Vector Auto Regression applicable only for Multivariate Time Series!')\n",
    "    \n",
    "    return stockprices,RMSE_DF,DWTable,GrangersMatrix,JohansonCointTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockprices,RMSE_DF,DWTable,GrangersMatrix,JohansonCointTable=vectorAutoRegression(start_date,end_date,symbols,portfolioWeights,portfolioValue,forecast_end_date,backtest_duration,\"345678\",\"vectorautoreg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockprices.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, url_for, request, session\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0\n",
    "\n",
    "@app.route('/')\n",
    "def start():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/results', methods = ['POST'])\n",
    "def results():\n",
    "    imagecounter = str(random.randint(10000,99999))\n",
    "    mainSelection=request.form['mainSelection']\n",
    "    tickerPortfolio=request.form['tickerPortfolio']\n",
    "    portfolioWeights=request.form['portfolioWeights']\n",
    "    portfolioValue=request.form.get('portfolioValue',type=float)\n",
    "    StartDate = request.form['StartDate'] \n",
    "    EndDate=request.form['EndDate'] \n",
    "    ForecastDate=request.form['ForecastDate'] \n",
    "    BacktestDays=int(request.form['BacktestDays'])\n",
    "    NoOfIter= int(request.form['NoOfIter'])\n",
    "    PercentileRange= request.form['PercentileRange']\n",
    "    averageType=request.form['AverageType']\n",
    "    WindowSize1=int(request.form['WindowSize1'])\n",
    "    WindowSize2=int(request.form['WindowSize2'])\n",
    "    WindowSize3=int(request.form['WindowSize3'])\n",
    "    noOfStdDev=int(request.form['noOfStdDev'])\n",
    "    riskfreerate=request.form.get('riskfreerate',type=float)\n",
    "    simMethod=request.form['simMethod']\n",
    "    seasonality=int(request.form['seasonality'])\n",
    "\n",
    "    if mainSelection==\"candlestickChart\":\n",
    "        clear_cache('candlesticks')\n",
    "        symbols=tickerPortfolio.split(\",\")\n",
    "        plot_candlesticks(symbols,StartDate,EndDate,imagecounter,\"candlesticks\")\n",
    "        \n",
    "        hists = os.listdir('static/candlesticks')\n",
    "        hists = ['candlesticks/' + file for file in hists]\n",
    "        \n",
    "        analysistype=\"Candlestick Chart\"      \n",
    "        \n",
    "        return render_template('results.html',analysistype=analysistype, hists = hists, mainSelection=mainSelection,\\\n",
    "                               tickerPortfolio=tickerPortfolio,portfolioWeights=portfolioWeights,\\\n",
    "                               portfolioValue=portfolioValue,StartDate=StartDate,EndDate=EndDate,ForecastDate=ForecastDate,\\\n",
    "                               BacktestDays=BacktestDays,NoOfIter=NoOfIter,PercentileRange=PercentileRange,\\\n",
    "                               averageType=averageType,WindowSize1=WindowSize1,WindowSize2=WindowSize2,WindowSize3=WindowSize3,riskfreerate=riskfreerate,simMethod=simMethod\\\n",
    "                               )\n",
    "    \n",
    "    elif mainSelection==\"RSIChart\":\n",
    "        clear_cache('relativestrengthindex')\n",
    "        symbols=tickerPortfolio.split(\",\")\n",
    "        plot_RSI(symbols,StartDate,EndDate,WindowSize1,averageType,imagecounter,\"relativestrengthindex\")\n",
    "        \n",
    "        hists = os.listdir('static/relativestrengthindex')\n",
    "        hists = ['relativestrengthindex/' + file for file in hists]\n",
    "        \n",
    "        analysistype=\"Relative Strength Index Using \"+averageType+\" With Period Of \"+str(WindowSize1)+\" Days\"      \n",
    "        \n",
    "        return render_template('results.html',analysistype=analysistype, hists = hists, mainSelection=mainSelection,\\\n",
    "                               tickerPortfolio=tickerPortfolio,portfolioWeights=portfolioWeights,\\\n",
    "                               portfolioValue=portfolioValue,StartDate=StartDate,EndDate=EndDate,ForecastDate=ForecastDate,\\\n",
    "                               BacktestDays=BacktestDays,NoOfIter=NoOfIter,PercentileRange=PercentileRange,\\\n",
    "                               averageType=averageType,WindowSize1=WindowSize1,WindowSize2=WindowSize2,WindowSize3=WindowSize3,riskfreerate=riskfreerate,simMethod=simMethod\\\n",
    "                               )\n",
    "    \n",
    "    elif mainSelection==\"MACDChart\":\n",
    "        clear_cache('macd')\n",
    "        symbols=tickerPortfolio.split(\",\")\n",
    "        plot_MACD(StartDate,EndDate,symbols,WindowSize1,WindowSize2,WindowSize3,imagecounter,\"macd\")     \n",
    "        \n",
    "        hists = os.listdir('static/macd')\n",
    "        hists = ['macd/' + file for file in hists]\n",
    "        \n",
    "        analysistype=\"Moving Average Convergence Divergence Using \"+averageType+\" With Periods Of \"+str(WindowSize1)+\" & \"+str(WindowSize2)+\" Days\"      \n",
    "        \n",
    "        return render_template('results.html',analysistype=analysistype, hists = hists, mainSelection=mainSelection,\\\n",
    "                               tickerPortfolio=tickerPortfolio,portfolioWeights=portfolioWeights,\\\n",
    "                               portfolioValue=portfolioValue,StartDate=StartDate,EndDate=EndDate,ForecastDate=ForecastDate,\\\n",
    "                               BacktestDays=BacktestDays,NoOfIter=NoOfIter,PercentileRange=PercentileRange,\\\n",
    "                               averageType=averageType,WindowSize1=WindowSize1,WindowSize2=WindowSize2,WindowSize3=WindowSize3,riskfreerate=riskfreerate,simMethod=simMethod\\\n",
    "                               )\n",
    "    \n",
    "    elif mainSelection==\"BBChart\":\n",
    "        clear_cache('bollingerbands')\n",
    "        symbols=tickerPortfolio.split(\",\")\n",
    "        plot_bollingerbands(symbols,StartDate,EndDate,WindowSize1,noOfStdDev,imagecounter,'bollingerbands')\n",
    "        \n",
    "        hists = os.listdir('static/bollingerbands')\n",
    "        hists = ['bollingerbands/' + file for file in hists]\n",
    "        \n",
    "        analysistype=\"Bollinger Bands Using SMA Period Of \"+str(WindowSize1)+\" Days & Bands Reflecting \"+str(noOfStdDev)+\" Std Dev-s\"      \n",
    "        \n",
    "        return render_template('results.html',analysistype=analysistype, hists = hists, mainSelection=mainSelection,\\\n",
    "                               tickerPortfolio=tickerPortfolio,portfolioWeights=portfolioWeights,\\\n",
    "                               portfolioValue=portfolioValue,StartDate=StartDate,EndDate=EndDate,ForecastDate=ForecastDate,\\\n",
    "                               BacktestDays=BacktestDays,NoOfIter=NoOfIter,PercentileRange=PercentileRange,\\\n",
    "                               averageType=averageType,WindowSize1=WindowSize1,WindowSize2=WindowSize2,WindowSize3=WindowSize3,riskfreerate=riskfreerate,simMethod=simMethod\\\n",
    "                               )\n",
    "    \n",
    "    elif mainSelection==\"MovingAve\":\n",
    "        clear_cache('movingaverage')\n",
    "        symbols=tickerPortfolio.split(\",\")\n",
    "        portfolioWeights_func=portfolioWeights.split(\",\")\n",
    "        portfolioWeights_func=[float(x) * 0.01 for x in portfolioWeights_func]\n",
    "                \n",
    "        T=np.busday_count(EndDate,ForecastDate)+BacktestDays\n",
    "        N=T+1\n",
    "        \n",
    "        plotpiechart(symbols,portfolioWeights_func,imagecounter,\"movingaverage\")\n",
    "\n",
    "        resultantDF, RMSE_DF, share_split_table=movingaverageforecast(StartDate,EndDate,BacktestDays,symbols,portfolioWeights_func,\\\n",
    "                                                   portfolioValue,T,N,averageType,WindowSize1,imagecounter,\"movingaverage\")        \n",
    "        \n",
    "        tableOne=RMSE_DF\n",
    "        \n",
    "        tailsDF=pd.DataFrame()\n",
    "        for i in range(0,len(resultantDF)):\n",
    "            tailsDF=pd.concat([tailsDF,resultantDF[i].tail(60)],axis=1)\n",
    "        tableTwo=tailsDF\n",
    "        tableThree=share_split_table\n",
    "        \n",
    "        hists = os.listdir('static/movingaverage')\n",
    "        hists = ['movingaverage/' + file for file in hists]\n",
    "        \n",
    "        analysistype=\"Moving Average Forecast Using \"+averageType+\" With Period Of \"+str(WindowSize1)+\" Days\"      \n",
    "        \n",
    "        return render_template('results.html',analysistype=analysistype, hists = hists, mainSelection=mainSelection,\\\n",
    "                               tickerPortfolio=tickerPortfolio,portfolioWeights=portfolioWeights,\\\n",
    "                               portfolioValue=portfolioValue,StartDate=StartDate,EndDate=EndDate,ForecastDate=ForecastDate,\\\n",
    "                               BacktestDays=BacktestDays,NoOfIter=NoOfIter,PercentileRange=PercentileRange,\\\n",
    "                               averageType=averageType,WindowSize1=WindowSize1,WindowSize2=WindowSize2,WindowSize3=WindowSize3,riskfreerate=riskfreerate,simMethod=simMethod,\\\n",
    "                              tableOne=[tableOne.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableTwo=[tableTwo.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableThree=[tableThree.to_html(classes='data', header=\"true\")])\n",
    "\n",
    "    \n",
    "    elif mainSelection==\"ARIMAGARCH\":\n",
    "        clear_cache('arimagarch')\n",
    "        symbols=tickerPortfolio.split(\",\")\n",
    "        portfolioWeights_func=portfolioWeights.split(\",\")\n",
    "        portfolioWeights_func=[float(x) * 0.01 for x in portfolioWeights_func]\n",
    "                \n",
    "        T=np.busday_count(EndDate,ForecastDate)+BacktestDays\n",
    "        N=T+1\n",
    "        \n",
    "        plotpiechart(symbols,portfolioWeights_func,imagecounter,\"arimagarch\")\n",
    "        \n",
    "        resultantDF, RMSE_DF, share_split_table =UnivarArimaGarchPredict(StartDate,EndDate,BacktestDays,\\\n",
    "                                                      symbols,portfolioWeights_func,portfolioValue,T,N,imagecounter,\"arimagarch\")\n",
    "        \n",
    "        \n",
    "        tableOne=RMSE_DF\n",
    "        tailsDF=pd.DataFrame()\n",
    "        for i in range(0,len(resultantDF)):\n",
    "            tailsDF=pd.concat([tailsDF,resultantDF[i].tail(60)],axis=1)\n",
    "        tableTwo=tailsDF\n",
    "        tableThree=share_split_table\n",
    "        \n",
    "        hists = os.listdir('static/arimagarch')\n",
    "        hists = ['arimagarch/' + file for file in hists]\n",
    "        \n",
    "        analysistype=\"Univariate Auto-ARIMA + GARCH(1,1) Forecast\"      \n",
    "        \n",
    "        return render_template('results.html',analysistype=analysistype, hists = hists, mainSelection=mainSelection,\\\n",
    "                               tickerPortfolio=tickerPortfolio,portfolioWeights=portfolioWeights,\\\n",
    "                               portfolioValue=portfolioValue,StartDate=StartDate,EndDate=EndDate,ForecastDate=ForecastDate,\\\n",
    "                               BacktestDays=BacktestDays,NoOfIter=NoOfIter,PercentileRange=PercentileRange,\\\n",
    "                               averageType=averageType,WindowSize1=WindowSize1,WindowSize2=WindowSize2,WindowSize3=WindowSize3,riskfreerate=riskfreerate,simMethod=simMethod,\\\n",
    "                              tableOne=[tableOne.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableTwo=[tableTwo.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableThree=[tableThree.to_html(classes='data', header=\"true\")])\n",
    "\n",
    "\n",
    "    elif mainSelection==\"HoltWinters\":\n",
    "        clear_cache('holtwinters')\n",
    "        symbols=tickerPortfolio.split(\",\")\n",
    "        portfolioWeights_func=portfolioWeights.split(\",\")\n",
    "        portfolioWeights_func=[float(x) * 0.01 for x in portfolioWeights_func]\n",
    "                \n",
    "        T=np.busday_count(EndDate,ForecastDate)+BacktestDays\n",
    "        N=T+1\n",
    "        \n",
    "        plotpiechart(symbols,portfolioWeights_func,imagecounter,\"holtwinters\")  \n",
    "        \n",
    "        FinalOutput,FinalOutputTailTable,BestConfigSummaryTable,OptimalRMSESummaryTable,OverallRMSEforConfigTable  =AutoHoltWinters(StartDate,EndDate,ForecastDate,BacktestDays,\\\n",
    "                                                        symbols,portfolioWeights_func,portfolioValue,seasonality,imagecounter,'holtwinters')\n",
    "        \n",
    "        tableOne=OptimalRMSESummaryTable\n",
    "        tableTwo=FinalOutputTailTable\n",
    "        tableThree=BestConfigSummaryTable\n",
    "        \n",
    "        hists = os.listdir('static/holtwinters')\n",
    "        hists = ['holtwinters/' + file for file in hists]\n",
    "        \n",
    "        analysistype=\"Triple Exponential Smoothing\"      \n",
    "        \n",
    "        return render_template('results.html',analysistype=analysistype, hists = hists, mainSelection=mainSelection,\\\n",
    "                               tickerPortfolio=tickerPortfolio,portfolioWeights=portfolioWeights,\\\n",
    "                               portfolioValue=portfolioValue,StartDate=StartDate,EndDate=EndDate,ForecastDate=ForecastDate,\\\n",
    "                               BacktestDays=BacktestDays,NoOfIter=NoOfIter,PercentileRange=PercentileRange,\\\n",
    "                               averageType=averageType,WindowSize1=WindowSize1,WindowSize2=WindowSize2,WindowSize3=WindowSize3,riskfreerate=riskfreerate,simMethod=simMethod,\\\n",
    "                              tableOne=[tableOne.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableTwo=[tableTwo.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableThree=[tableThree.to_html(classes='data', header=\"true\")])\n",
    "    \n",
    "    \n",
    "    elif mainSelection==\"VecAR\":\n",
    "        clear_cache('vectorautoreg')\n",
    "        symbols=tickerPortfolio.split(\",\")\n",
    "        portfolioWeights_func=portfolioWeights.split(\",\")\n",
    "        portfolioWeights_func=[float(x) * 0.01 for x in portfolioWeights_func]\n",
    "        \n",
    "        plotpiechart(symbols,portfolioWeights_func,imagecounter,\"vectorautoreg\")\n",
    "        \n",
    "        stockprices,RMSE_DF,DWTable,GrangersMatrix,JohansonCointTable=vectorAutoRegression(StartDate,EndDate,\\\n",
    "                        symbols,portfolioWeights_func,portfolioValue,ForecastDate,BacktestDays,imagecounter,\"vectorautoreg\")\n",
    "\n",
    "\n",
    "        \n",
    "        tableOne=RMSE_DF\n",
    "        tableTwo=stockprices.tail(60)\n",
    "        tableThree=GrangersMatrix\n",
    "        tableFour=JohansonCointTable\n",
    "        tableFive=DWTable\n",
    "        \n",
    "        hists = os.listdir('static/vectorautoreg')\n",
    "        hists = ['vectorautoreg/' + file for file in hists]\n",
    "        \n",
    "        if len(symbols)>1:\n",
    "            analysistype=\"Multivariate Vector Auto Regression Forecast\"\n",
    "        elif len(symbols)==1:\n",
    "            analysistype=\"ERROR ! Vector Auto Regression Forecast Requires Multi Variate Time Series Data\"\n",
    "        \n",
    "        return render_template('resultsVecar.html',analysistype=analysistype, hists = hists, mainSelection=mainSelection,\\\n",
    "                               tickerPortfolio=tickerPortfolio,portfolioWeights=portfolioWeights,\\\n",
    "                               portfolioValue=portfolioValue,StartDate=StartDate,EndDate=EndDate,ForecastDate=ForecastDate,\\\n",
    "                               BacktestDays=BacktestDays,NoOfIter=NoOfIter,PercentileRange=PercentileRange,\\\n",
    "                               averageType=averageType,WindowSize1=WindowSize1,WindowSize2=WindowSize2,WindowSize3=WindowSize3,riskfreerate=riskfreerate,simMethod=simMethod,\\\n",
    "                              tableOne=[tableOne.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableTwo=[tableTwo.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableThree=[tableThree.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableFour=[tableFour.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableFive=[tableFive.to_html(classes='data', header=\"true\")])\n",
    " \n",
    "        \n",
    "    elif mainSelection==\"returnsDist\":\n",
    "        clear_cache('analysis')\n",
    "        symbols=tickerPortfolio.split(\",\")\n",
    "        portfolioWeights_func=portfolioWeights.split(\",\")\n",
    "        portfolioWeights_func=[float(x) * 0.01 for x in portfolioWeights_func]\n",
    "        \n",
    "        dfprices, noOfShares, share_split_table = extract_prices(StartDate,EndDate,symbols,portfolioWeights_func,portfolioValue)\n",
    "        \n",
    "        dfreturns ,df_mean_stdev=calc_returns(dfprices,symbols)\n",
    "        \n",
    "        plotpiechart(symbols,portfolioWeights_func,imagecounter,\"analysis\")\n",
    "        plotprices(dfprices,symbols,imagecounter,\"analysis\")\n",
    "        plotreturns(dfreturns,imagecounter,\"analysis\")\n",
    "        KSTestResultsDF, ShapiroWilkTestResultsDF , Kurtosis_Skew, ADFTestResultsDF = fit_test_normal(dfreturns,symbols,imagecounter,\"analysis\")\n",
    "        StartMidEndDF =compareStartMidEnd(dfreturns,df_mean_stdev)\n",
    "        \n",
    "        hists = os.listdir('static/analysis')\n",
    "        hists = ['analysis/' + file for file in hists]\n",
    "        \n",
    "        tableOne=df_mean_stdev\n",
    "        tableTwo=KSTestResultsDF\n",
    "        tableThree=Kurtosis_Skew\n",
    "        tableFour=dfreturns.corr()\n",
    "        tableFive=ADFTestResultsDF\n",
    "        tableSix=StartMidEndDF\n",
    "        analysistype=\"Detailed Analysis Of Log Returns\"      \n",
    "        \n",
    "        \n",
    "        return render_template('resultsLogreturns.html',analysistype=analysistype, hists = hists, mainSelection=mainSelection,\\\n",
    "                               tickerPortfolio=tickerPortfolio,portfolioWeights=portfolioWeights,\\\n",
    "                               portfolioValue=portfolioValue,StartDate=StartDate,EndDate=EndDate,ForecastDate=ForecastDate,\\\n",
    "                               BacktestDays=BacktestDays,NoOfIter=NoOfIter,PercentileRange=PercentileRange,\\\n",
    "                               averageType=averageType,WindowSize1=WindowSize1,WindowSize2=WindowSize2,WindowSize3=WindowSize3,riskfreerate=riskfreerate,simMethod=simMethod,\\\n",
    "                              tableOne=[tableOne.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableTwo=[tableTwo.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableThree=[tableThree.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableFour=[tableFour.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableFive=[tableFive.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableSix=[tableSix.to_html(classes='data', header=\"true\")])\n",
    "        \n",
    "    elif mainSelection==\"GBM\":\n",
    "        clear_cache('gbm_bootstrap')\n",
    "        symbols=tickerPortfolio.split(\",\")\n",
    "        portfolioWeights_func=portfolioWeights.split(\",\")\n",
    "        portfolioWeights_func=[float(x) * 0.01 for x in portfolioWeights_func]\n",
    "        \n",
    "        plotpiechart(symbols,portfolioWeights_func,imagecounter,\"gbm_bootstrap\")\n",
    "        \n",
    "        T=np.busday_count(EndDate,ForecastDate)+BacktestDays\n",
    "        N=T+1\n",
    "        \n",
    "        final, share_split_tableFULL , dfreturns , df_mean_stdev , ReturnsAtForecastEndDate, dfprices = MonteCarlo_GBM(StartDate,EndDate,BacktestDays,PercentileRange,symbols,\\\n",
    "                       portfolioWeights_func,portfolioValue,T,N,NoOfIter,imagecounter,\"gbm_bootstrap\")\n",
    "        \n",
    "        if BacktestDays>0:\n",
    "            tableOne=calculateRMSE(final,T,BacktestDays,dfprices)\n",
    "            \n",
    "        elif BacktestDays<=0:\n",
    "            tableOne=pd.DataFrame()\n",
    "                    \n",
    "        hists = os.listdir('static/gbm_bootstrap')\n",
    "        hists = ['gbm_bootstrap/' + file for file in hists]\n",
    "        \n",
    "        GBMtail=pd.concat([final[final.columns[0:(len(symbols)+1)]].tail(60),final[final.columns[-3*(len(symbols)+1):]].tail(60)],axis=1)\n",
    "        GBMtail=GBMtail.round(2) \n",
    "        \n",
    "        tableTwo=GBMtail\n",
    "        tableThree=ReturnsAtForecastEndDate\n",
    "        tableFour=dfreturns.corr()\n",
    "        tableFive=share_split_tableFULL\n",
    "        \n",
    "        analysistype=\"Forecast Of Future Price Values For Portfolio Based On Geometric Brownian Motion\"\n",
    "        \n",
    "        return render_template('results.html',analysistype=analysistype, hists = hists, mainSelection=mainSelection,\\\n",
    "                               tickerPortfolio=tickerPortfolio,portfolioWeights=portfolioWeights,\\\n",
    "                               portfolioValue=portfolioValue,StartDate=StartDate,EndDate=EndDate,ForecastDate=ForecastDate,\\\n",
    "                               BacktestDays=BacktestDays,NoOfIter=NoOfIter,PercentileRange=PercentileRange,\\\n",
    "                               averageType=averageType,WindowSize1=WindowSize1,WindowSize2=WindowSize2,WindowSize3=WindowSize3,riskfreerate=riskfreerate,simMethod=simMethod,\\\n",
    "                              tableOne=[tableOne.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableTwo=[tableTwo.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableThree=[tableThree.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableFour=[tableFour.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableFive=[tableFive.to_html(classes='data', header=\"true\")])\n",
    "    \n",
    "   \n",
    "    elif mainSelection==\"Bootstrap\":\n",
    "        clear_cache('gbm_bootstrap')\n",
    "        symbols=tickerPortfolio.split(\",\")\n",
    "        portfolioWeights_func=portfolioWeights.split(\",\")\n",
    "        portfolioWeights_func=[float(x) * 0.01 for x in portfolioWeights_func]\n",
    "        \n",
    "        plotpiechart(symbols,portfolioWeights_func,imagecounter,\"gbm_bootstrap\")\n",
    "                \n",
    "        T=np.busday_count(EndDate,ForecastDate)+BacktestDays\n",
    "        N=T+1\n",
    "        \n",
    "        final, share_split_tableFULL , dfreturns , df_mean_stdev , ReturnsAtForecastEndDate,dfprices = MonteCarlo_Bootstrap(StartDate,EndDate,BacktestDays,PercentileRange,symbols,\\\n",
    "                       portfolioWeights_func,portfolioValue,T,N,NoOfIter,imagecounter,\"gbm_bootstrap\")\n",
    "        \n",
    "        if BacktestDays>0:\n",
    "            tableOne=calculateRMSE(final,T,BacktestDays,dfprices)\n",
    "            \n",
    "        elif BacktestDays<=0:\n",
    "            tableOne=pd.DataFrame()\n",
    "            \n",
    "        hists = os.listdir('static/gbm_bootstrap')\n",
    "        hists = ['gbm_bootstrap/' + file for file in hists]\n",
    "        \n",
    "        Bootstraptail=pd.concat([final[final.columns[0:(len(symbols)+1)]].tail(60),final[final.columns[-3*(len(symbols)+1):]].tail(60)],axis=1)\n",
    "        Bootstraptail=Bootstraptail.round(2) \n",
    "        \n",
    "        tableTwo=Bootstraptail\n",
    "        tableThree=ReturnsAtForecastEndDate\n",
    "        tableFour=dfreturns.corr()\n",
    "        tableFive=share_split_tableFULL\n",
    "        \n",
    "        analysistype=\"Forecast Of Future Price Values For Portfolio Based On Bootstrap Sampling\"\n",
    "        \n",
    "        return render_template('results.html',analysistype=analysistype, hists = hists, mainSelection=mainSelection,\\\n",
    "                               tickerPortfolio=tickerPortfolio,portfolioWeights=portfolioWeights,\\\n",
    "                               portfolioValue=portfolioValue,StartDate=StartDate,EndDate=EndDate,ForecastDate=ForecastDate,\\\n",
    "                               BacktestDays=BacktestDays,NoOfIter=NoOfIter,PercentileRange=PercentileRange,\\\n",
    "                               averageType=averageType,WindowSize1=WindowSize1,WindowSize2=WindowSize2,WindowSize3=WindowSize3,riskfreerate=riskfreerate,simMethod=simMethod,\\\n",
    "                              tableOne=[tableOne.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableTwo=[tableTwo.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableThree=[tableThree.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableFour=[tableFour.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableFive=[tableFive.to_html(classes='data', header=\"true\")])\n",
    "    \n",
    "    elif mainSelection==\"EfficientFrontier\":\n",
    "        clear_cache('efficientportfolio')\n",
    "        symbols=tickerPortfolio.split(\",\")\n",
    "        \n",
    "        T=np.busday_count(EndDate,ForecastDate)+0\n",
    "        N=T+1\n",
    "                \n",
    "        FinalResultsTable,SharpeDetail,SortinoDetail=EfficientPortfolio(StartDate,EndDate,symbols,portfolioValue,T,N,NoOfIter,riskfreerate,simMethod,imagecounter,\"efficientportfolio\")\n",
    "       \n",
    "        tableTwo=FinalResultsTable.nlargest(1,['Sharpe Ratio'])\n",
    "        SharpeWeights=FinalResultsTable.nlargest(1,['Sharpe Ratio'])['Weights'].tolist()\n",
    "        print(SharpeWeights[0])\n",
    "      \n",
    "        labels = symbols\n",
    "        sizes = SharpeWeights[0]\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        ax1.pie(SharpeWeights[0], labels=symbols, autopct='%1.1f%%',\n",
    "            shadow=True, startangle=90)\n",
    "        ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "        plt.title(\"Optimal Portfolio Weights Based On Sharpe Ratio\")\n",
    "        plt.savefig(f'static/efficientportfolio/{imagecounter}_01SharpePortfolioweights.png')\n",
    "            \n",
    "        tableThree=FinalResultsTable.nlargest(1,['Sortino Ratio'])\n",
    "        SortinoWeights=FinalResultsTable.nlargest(1,['Sortino Ratio'])['Weights'].tolist()\n",
    "        print(SortinoWeights[0])\n",
    "        \n",
    "        sizes = SortinoWeights[0]\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        ax1.pie(SortinoWeights[0], labels=symbols, autopct='%1.1f%%',\n",
    "            shadow=True, startangle=90)\n",
    "        ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "        plt.title(\"Optimal Portfolio Weights Based On Sortino Ratio\")\n",
    "        plt.savefig(f'static/efficientportfolio/{imagecounter}_01SortinoPortfolioweights.png')\n",
    "        \n",
    "        hists = os.listdir('static/efficientportfolio')\n",
    "        hists = ['efficientportfolio/' + file for file in hists]\n",
    "        \n",
    "        analysistype=\"Efficient Portfolio Weights Via Mean-Variance Analysis\" \n",
    "        subheading=\"For Forecast Period Of \"+str(N)+\" Days From \"+EndDate+\" Till \"+ForecastDate+\" Via \"+simMethod+\" Simulation Over \"+str(NoOfIter)+\" Iterations\"\n",
    "        \n",
    "        return render_template('results.html',SharpeDetail=SharpeDetail,SortinoDetail=SortinoDetail,subheading=subheading,analysistype=analysistype, hists = hists, mainSelection=mainSelection,\\\n",
    "                               tickerPortfolio=tickerPortfolio,portfolioWeights=portfolioWeights,\\\n",
    "                               portfolioValue=portfolioValue,StartDate=StartDate,EndDate=EndDate,ForecastDate=ForecastDate,\\\n",
    "                               BacktestDays=BacktestDays,NoOfIter=NoOfIter,PercentileRange=PercentileRange,\\\n",
    "                               averageType=averageType,WindowSize1=WindowSize1,WindowSize2=WindowSize2,WindowSize3=WindowSize3,riskfreerate=riskfreerate,simMethod=simMethod,\\\n",
    "                              tableTwo=[tableTwo.to_html(classes='data', header=\"true\")],\\\n",
    "                              tableThree=[tableThree.to_html(classes='data', header=\"true\")])\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, use_reloader=False, port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
